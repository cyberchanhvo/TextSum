{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T12:06:46.761118Z",
     "start_time": "2021-07-20T12:06:44.413368Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-extractive-summarizer in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.7.1)\n",
      "Requirement already satisfied: spacy in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from bert-extractive-summarizer) (3.1.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from bert-extractive-summarizer) (0.24.2)\n",
      "Requirement already satisfied: transformers in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from bert-extractive-summarizer) (4.8.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn->bert-extractive-summarizer) (1.0.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn->bert-extractive-summarizer) (1.7.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn->bert-extractive-summarizer) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from scikit-learn->bert-extractive-summarizer) (1.19.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy->bert-extractive-summarizer) (2.0.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy->bert-extractive-summarizer) (0.8.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy->bert-extractive-summarizer) (1.8.2)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from spacy->bert-extractive-summarizer) (3.7.4.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy->bert-extractive-summarizer) (2.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy->bert-extractive-summarizer) (0.6.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy->bert-extractive-summarizer) (47.1.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy->bert-extractive-summarizer) (3.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy->bert-extractive-summarizer) (4.61.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy->bert-extractive-summarizer) (2.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy->bert-extractive-summarizer) (21.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy->bert-extractive-summarizer) (2.4.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy->bert-extractive-summarizer) (3.0.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy->bert-extractive-summarizer) (0.7.4)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy->bert-extractive-summarizer) (0.3.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.8 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy->bert-extractive-summarizer) (8.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy->bert-extractive-summarizer) (3.0.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from catalogue<2.1.0,>=2.0.4->spacy->bert-extractive-summarizer) (3.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from packaging>=20.0->spacy->bert-extractive-summarizer) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pathy>=0.3.5->spacy->bert-extractive-summarizer) (5.1.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2.0.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (1.26.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy->bert-extractive-summarizer) (0.4.4)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy->bert-extractive-summarizer) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jinja2->spacy->bert-extractive-summarizer) (2.0.1)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers->bert-extractive-summarizer) (0.0.45)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers->bert-extractive-summarizer) (2021.7.6)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers->bert-extractive-summarizer) (0.10.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers->bert-extractive-summarizer) (3.0.12)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers->bert-extractive-summarizer) (3.10.1)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers->bert-extractive-summarizer) (0.0.12)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers->bert-extractive-summarizer) (5.4.1)\n",
      "Requirement already satisfied: six in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from sacremoses->transformers->bert-extractive-summarizer) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install werkzeug\n",
    "!pip install bert-extractive-summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T12:07:22.516928Z",
     "start_time": "2021-07-20T12:07:05.004551Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyter-client==6.1.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (6.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jupyter-client==6.1.5) (2.8.2)\n",
      "Requirement already satisfied: tornado>=4.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jupyter-client==6.1.5) (6.1)\n",
      "Requirement already satisfied: traitlets in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jupyter-client==6.1.5) (5.0.0)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jupyter-client==6.1.5) (4.7.1)\n",
      "Requirement already satisfied: pyzmq>=13 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jupyter-client==6.1.5) (22.1.0)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jupyter-core>=4.6.0->jupyter-client==6.1.5) (301)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from python-dateutil>=2.1->jupyter-client==6.1.5) (1.15.0)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from traitlets->jupyter-client==6.1.5) (0.2.0)\n",
      "Requirement already satisfied: wrapt==1.12.1 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (1.12.1)\n",
      "Requirement already satisfied: traitlets==5.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (5.0.0)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from traitlets==5.0) (0.2.0)\n",
      "Requirement already satisfied: typed-ast in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.4.3)\n",
      "Requirement already satisfied: pytest-filter-subpackage in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: pytest>=3.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pytest-filter-subpackage) (6.2.4)\n",
      "Requirement already satisfied: py>=1.8.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pytest>=3.0->pytest-filter-subpackage) (1.10.0)\n",
      "Requirement already satisfied: pluggy<1.0.0a1,>=0.12 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pytest>=3.0->pytest-filter-subpackage) (0.13.1)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pytest>=3.0->pytest-filter-subpackage) (1.4.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pytest>=3.0->pytest-filter-subpackage) (21.2.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pytest>=3.0->pytest-filter-subpackage) (3.10.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pytest>=3.0->pytest-filter-subpackage) (0.4.4)\n",
      "Requirement already satisfied: toml in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pytest>=3.0->pytest-filter-subpackage) (0.10.2)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pytest>=3.0->pytest-filter-subpackage) (1.1.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pytest>=3.0->pytest-filter-subpackage) (21.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from importlib-metadata>=0.12->pytest>=3.0->pytest-filter-subpackage) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata>=0.12->pytest>=3.0->pytest-filter-subpackage) (3.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from packaging->pytest>=3.0->pytest-filter-subpackage) (2.4.7)\n",
      "Requirement already satisfied: pytest-cov in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (2.12.1)\n",
      "Requirement already satisfied: toml in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pytest-cov) (0.10.2)\n",
      "Requirement already satisfied: coverage>=5.2.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pytest-cov) (5.5)\n",
      "Requirement already satisfied: pytest>=4.6 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pytest-cov) (6.2.4)\n",
      "Requirement already satisfied: pluggy<1.0.0a1,>=0.12 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pytest>=4.6->pytest-cov) (0.13.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pytest>=4.6->pytest-cov) (0.4.4)\n",
      "Requirement already satisfied: py>=1.8.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pytest>=4.6->pytest-cov) (1.10.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pytest>=4.6->pytest-cov) (21.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pytest>=4.6->pytest-cov) (1.4.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pytest>=4.6->pytest-cov) (21.2.0)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pytest>=4.6->pytest-cov) (1.1.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pytest>=4.6->pytest-cov) (3.10.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from importlib-metadata>=0.12->pytest>=4.6->pytest-cov) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata>=0.12->pytest>=4.6->pytest-cov) (3.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from packaging->pytest>=4.6->pytest-cov) (2.4.7)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (2.5.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow) (3.17.3)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: tensorboard~=2.5 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow) (1.34.1)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow) (0.13.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: cached-property in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from tensorboard~=2.5->tensorflow) (0.4.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (47.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from tensorboard~=2.5->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from tensorboard~=2.5->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from tensorboard~=2.5->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from tensorboard~=2.5->tensorflow) (1.33.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from tensorboard~=2.5->tensorflow) (2.0.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow) (3.10.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (3.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow) (3.5.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyvi in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: sklearn-crfsuite in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pyvi) (0.3.6)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pyvi) (0.24.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn->pyvi) (1.7.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn->pyvi) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from scikit-learn->pyvi) (1.19.5)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn->pyvi) (1.0.1)\n",
      "Requirement already satisfied: tqdm>=2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from sklearn-crfsuite->pyvi) (4.61.2)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from sklearn-crfsuite->pyvi) (0.9.7)\n",
      "Requirement already satisfied: six in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from sklearn-crfsuite->pyvi) (1.15.0)\n",
      "Requirement already satisfied: tabulate in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from sklearn-crfsuite->pyvi) (0.8.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tqdm>=2.0->sklearn-crfsuite->pyvi) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install jupyter-client==6.1.5\n",
    "!pip install wrapt==1.12.1\n",
    "!pip install traitlets==5.0\n",
    "!pip install typed-ast\n",
    "!pip install pytest-filter-subpackage\n",
    "!pip install pytest-cov\n",
    "!pip install tensorflow --user\n",
    "!pip install pyvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T12:10:07.382630Z",
     "start_time": "2021-07-20T12:07:37.944857Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 1)) (1.19.5)\n",
      "Collecting torch\n",
      "  Downloading torch-1.9.0-cp37-cp37m-win_amd64.whl (222.0 MB)\n",
      "Collecting spacy==2.1.3\n",
      "  Downloading spacy-2.1.3-cp37-cp37m-win_amd64.whl (26.9 MB)\n",
      "Collecting transformers==3.3.0\n",
      "  Using cached transformers-3.3.0-py3-none-any.whl (1.1 MB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp37-cp37m-win_amd64.whl (1.1 MB)\n",
      "Collecting Cython\n",
      "  Downloading Cython-0.29.24-cp37-cp37m-win_amd64.whl (1.6 MB)\n",
      "Collecting tqdm==4.32.2\n",
      "  Downloading tqdm-4.32.2-py2.py3-none-any.whl (50 kB)\n",
      "Collecting neuralcoref\n",
      "  Downloading neuralcoref-4.0-cp37-cp37m-win_amd64.whl (227 kB)\n",
      "Collecting argparse\n",
      "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from -r requirements.txt (line 10)) (0.24.2)\n",
      "Requirement already satisfied: pytest in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from -r requirements.txt (line 11)) (6.2.4)\n",
      "Collecting blis<0.3.0,>=0.2.2\n",
      "  Downloading blis-0.2.4-cp37-cp37m-win_amd64.whl (3.1 MB)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy==2.1.3->-r requirements.txt (line 3)) (2.0.5)\n",
      "Collecting plac<1.0.0,>=0.9.6\n",
      "  Using cached plac-0.9.6-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy==2.1.3->-r requirements.txt (line 3)) (0.8.2)\n",
      "Collecting srsly<1.1.0,>=0.0.5\n",
      "  Downloading srsly-1.0.5-cp37-cp37m-win_amd64.whl (176 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy==2.1.3->-r requirements.txt (line 3)) (2.26.0)\n",
      "Collecting preshed<2.1.0,>=2.0.1\n",
      "  Downloading preshed-2.0.1-cp37-cp37m-win_amd64.whl (73 kB)\n",
      "Collecting jsonschema<3.0.0,>=2.6.0\n",
      "  Downloading jsonschema-2.6.0-py2.py3-none-any.whl (39 kB)\n",
      "Collecting thinc<7.1.0,>=7.0.2\n",
      "  Downloading thinc-7.0.8-cp37-cp37m-win_amd64.whl (1.9 MB)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy==2.1.3->-r requirements.txt (line 3)) (1.0.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers==3.3.0->-r requirements.txt (line 4)) (2021.7.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers==3.3.0->-r requirements.txt (line 4)) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers==3.3.0->-r requirements.txt (line 4)) (0.0.45)\n",
      "Collecting tokenizers==0.8.1.rc2\n",
      "  Downloading tokenizers-0.8.1rc2-cp37-cp37m-win_amd64.whl (1.9 MB)\n",
      "Requirement already satisfied: packaging in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers==3.3.0->-r requirements.txt (line 4)) (21.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3->-r requirements.txt (line 3)) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3->-r requirements.txt (line 3)) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3->-r requirements.txt (line 3)) (2.0.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3->-r requirements.txt (line 3)) (1.26.6)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from torch->-r requirements.txt (line 2)) (3.7.4.3)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.18.2-py3-none-any.whl (131 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 10)) (1.0.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 10)) (1.7.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 10)) (2.2.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pytest->-r requirements.txt (line 11)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pytest->-r requirements.txt (line 11)) (21.2.0)\n",
      "Requirement already satisfied: py>=1.8.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pytest->-r requirements.txt (line 11)) (1.10.0)\n",
      "Requirement already satisfied: toml in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pytest->-r requirements.txt (line 11)) (0.10.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pytest->-r requirements.txt (line 11)) (3.10.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pytest->-r requirements.txt (line 11)) (0.4.4)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pytest->-r requirements.txt (line 11)) (1.1.1)\n",
      "Requirement already satisfied: pluggy<1.0.0a1,>=0.12 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pytest->-r requirements.txt (line 11)) (0.13.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata>=0.12->pytest->-r requirements.txt (line 11)) (3.5.0)\n",
      "Collecting s3transfer<0.6.0,>=0.5.0\n",
      "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
      "Collecting botocore<1.22.0,>=1.21.2\n",
      "  Downloading botocore-1.21.2-py3-none-any.whl (7.7 MB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from botocore<1.22.0,>=1.21.2->boto3->neuralcoref->-r requirements.txt (line 8)) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.2->boto3->neuralcoref->-r requirements.txt (line 8)) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from packaging->transformers==3.3.0->-r requirements.txt (line 4)) (2.4.7)\n",
      "Requirement already satisfied: click in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from sacremoses->transformers==3.3.0->-r requirements.txt (line 4)) (7.1.2)\n",
      "Installing collected packages: jmespath, tqdm, srsly, preshed, plac, botocore, blis, thinc, s3transfer, jsonschema, tokenizers, spacy, sentencepiece, boto3, transformers, torch, neuralcoref, Cython, argparse\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.61.2\n",
      "    Uninstalling tqdm-4.61.2:\n",
      "      Successfully uninstalled tqdm-4.61.2\n",
      "  Attempting uninstall: srsly\n",
      "    Found existing installation: srsly 2.4.1\n",
      "    Uninstalling srsly-2.4.1:\n",
      "      Successfully uninstalled srsly-2.4.1\n",
      "  Attempting uninstall: preshed\n",
      "    Found existing installation: preshed 3.0.5\n",
      "    Uninstalling preshed-3.0.5:\n",
      "      Successfully uninstalled preshed-3.0.5\n",
      "  Attempting uninstall: blis\n",
      "    Found existing installation: blis 0.7.4\n",
      "    Uninstalling blis-0.7.4:\n",
      "      Successfully uninstalled blis-0.7.4\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.0.8\n",
      "    Uninstalling thinc-8.0.8:\n",
      "      Successfully uninstalled thinc-8.0.8\n",
      "  Attempting uninstall: jsonschema\n",
      "    Found existing installation: jsonschema 3.2.0\n",
      "    Uninstalling jsonschema-3.2.0:\n",
      "      Successfully uninstalled jsonschema-3.2.0\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.10.3\n",
      "    Uninstalling tokenizers-0.10.3:\n",
      "      Successfully uninstalled tokenizers-0.10.3\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.1.1\n",
      "    Uninstalling spacy-3.1.1:\n",
      "      Successfully uninstalled spacy-3.1.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.8.2\n",
      "    Uninstalling transformers-4.8.2:\n",
      "      Successfully uninstalled transformers-4.8.2\n",
      "Successfully installed Cython-0.29.24 argparse-1.4.0 blis-0.2.4 boto3-1.18.2 botocore-1.21.2 jmespath-0.10.0 jsonschema-2.6.0 neuralcoref-4.0 plac-0.9.6 preshed-2.0.1 s3transfer-0.5.0 sentencepiece-0.1.96 spacy-2.1.3 srsly-1.0.5 thinc-7.0.8 tokenizers-0.8.1rc2 torch-1.9.0 tqdm-4.32.2 transformers-3.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T12:10:30.802284Z",
     "start_time": "2021-07-20T12:10:28.600146Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: neuralcoref in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (4.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from neuralcoref) (1.19.5)\n",
      "Requirement already satisfied: boto3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from neuralcoref) (1.18.2)\n",
      "Requirement already satisfied: spacy>=2.1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from neuralcoref) (2.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from neuralcoref) (2.26.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2.0.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (1.26.6)\n",
      "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (2.6.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (1.0.5)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (1.0.5)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (0.2.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (0.8.2)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (2.0.1)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (7.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (2.0.5)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (0.9.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from thinc<7.1.0,>=7.0.2->spacy>=2.1.0->neuralcoref) (4.32.2)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from boto3->neuralcoref) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from boto3->neuralcoref) (0.5.0)\n",
      "Requirement already satisfied: botocore<1.22.0,>=1.21.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from boto3->neuralcoref) (1.21.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from botocore<1.22.0,>=1.21.2->boto3->neuralcoref) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.2->boto3->neuralcoref) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install neuralcoref --no-binary neuralcoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-21T03:55:27.809344Z",
     "start_time": "2021-07-21T03:55:22.864570Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-metric\n",
      "  Downloading rouge_metric-1.0.1-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: rouge-metric\n",
      "Successfully installed rouge-metric-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge-metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T05:50:50.892791Z",
     "start_time": "2021-07-23T05:50:35.815087Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "from typing import Dict\n",
    "from transformers import (PreTrainedModel, PreTrainedTokenizer,\n",
    "                          BertModel, BertTokenizer,\n",
    "                          GPT2Model, GPT2Tokenizer,\n",
    "                          BartModel, BartTokenizer, \n",
    "                          OpenAIGPTModel, OpenAIGPTTokenizer, \n",
    "                          CTRLModel, CTRLTokenizer, \n",
    "                          TransfoXLModel, TransfoXLTokenizer, \n",
    "                          XLNetModel, XLNetTokenizer,\n",
    "                          XLMModel, XLMTokenizer, \n",
    "                          DistilBertModel, DistilBertTokenizer,\n",
    "                          AlbertModel, AlbertTokenizer,\n",
    "                          AutoModel, AutoTokenizer)\n",
    "\n",
    "from transformers import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import neuralcoref\n",
    "import spacy\n",
    "\n",
    "from numpy import ndarray\n",
    "from spacy.lang.vi import Vietnamese\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T05:50:50.911122Z",
     "start_time": "2021-07-23T05:50:50.892791Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from rouge_metric import PyRouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T05:50:52.304838Z",
     "start_time": "2021-07-23T05:50:50.915111Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Setup classes & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T05:50:52.493997Z",
     "start_time": "2021-07-23T05:50:52.466068Z"
    },
    "code_folding": [
     2,
     6
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "class BertParent(object):\n",
    "    \"\"\"\n",
    "    Base handler for BERT models.\n",
    "    \"\"\"\n",
    "    MODELS = {\n",
    "                'bert-base-multilingual-uncased': (BertModel, BertTokenizer),\n",
    "                'bert-base-uncased': (BertModel, BertTokenizer),\n",
    "                'bert-large-uncased': (BertModel, BertTokenizer),\n",
    "                'gpt2': (GPT2Model, GPT2Tokenizer),\n",
    "                'facebook/bart-large': (BartModel, BartTokenizer),\n",
    "                'openai-gpt': (OpenAIGPTModel, OpenAIGPTTokenizer),\n",
    "                'ctrl': (CTRLModel, CTRLTokenizer),\n",
    "                'transfo-xl-wt103': (TransfoXLModel, TransfoXLTokenizer), \n",
    "                'xlnet-large-cased': (XLNetModel, XLNetTokenizer),\n",
    "                'xlnet-base-cased': (XLNetModel, XLNetTokenizer),\n",
    "                'xlm-mlm-enfr-1024': (XLMModel, XLMTokenizer),\n",
    "                'distilbert-base-uncased': (DistilBertModel, DistilBertTokenizer),\n",
    "                'albert-base-v2': (AlbertModel, AlbertTokenizer),\n",
    "                'albert-large-v2': (AlbertModel, AlbertTokenizer),\n",
    "                'allenai/scibert_scivocab_uncased': (AutoModel, AutoTokenizer),\n",
    "                'vinai/phobert-large': (AutoModel, AutoTokenizer)\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                model: str,\n",
    "                custom_model: PreTrainedModel = None,\n",
    "                custom_tokenizer: PreTrainedTokenizer = None):\n",
    "        \"\"\"\n",
    "        :param model: Model is the string path for the bert weights. If given a keyword, the s3 path will be used.\n",
    "        :param custom_model: This is optional if a custom bert model is used.\n",
    "        :param custom_tokenizer: Place to use custom tokenizer.\n",
    "        \"\"\"\n",
    "        base_model, base_tokenizer = self.MODELS.get(model, (None, None))\n",
    "\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        if custom_model:\n",
    "            self.model = custom_model.to(self.device)\n",
    "        else:\n",
    "            self.model = base_model.from_pretrained(model, output_hidden_states=True).to(self.device)\n",
    "\n",
    "        if custom_tokenizer:\n",
    "            self.tokenizer = custom_tokenizer\n",
    "        else:\n",
    "            self.tokenizer = base_tokenizer.from_pretrained(model)\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    def tokenize_input(self, text: str) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Tokenizes the text input.\n",
    "        :param text: Text to tokenize.\n",
    "        :return: Returns a torch tensor.\n",
    "        \"\"\"\n",
    "        tokenized_text = self.tokenizer.tokenize(text)\n",
    "        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        return torch.tensor([indexed_tokens]).to(self.device) \n",
    "\n",
    "    def extract_embeddings(self,\n",
    "                        text: str,\n",
    "                        hidden: Union[List[int], int] = -2,\n",
    "                        reduce_option: str = 'mean',\n",
    "                        hidden_concat: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Extracts the embeddings for the given text.\n",
    "        :param text: The text to extract embeddings for.\n",
    "        :param hidden: The hidden layer(s) to use for a readout handler.\n",
    "        :param squeeze: If we should squeeze the outputs (required for some layers).\n",
    "        :param reduce_option: How we should reduce the items.\n",
    "        :param hidden_concat: Whether or not to concat multiple hidden layers.\n",
    "        :return: A torch vector.\n",
    "        \"\"\"\n",
    "        tokens_tensor = self.tokenize_input(text)\n",
    "        pooled, hidden_states = self.model(tokens_tensor)[-2:]\n",
    "\n",
    "        # deprecated temporary keyword functions.\n",
    "        \n",
    "        if reduce_option == 'concat_last_4':\n",
    "            last_4 = [hidden_states[i] for i in (-1, -2, -3, -4)]\n",
    "            cat_hidden_states = torch.cat(tuple(last_4), dim=-1)\n",
    "            return torch.mean(cat_hidden_states, dim=1).squeeze()\n",
    "\n",
    "        elif reduce_option == 'reduce_last_4':\n",
    "            last_4 = [hidden_states[i] for i in (-1, -2, -3, -4)]\n",
    "            return torch.cat(tuple(last_4), dim=1).mean(axis=1).squeeze()\n",
    "\n",
    "        elif type(hidden) == int:\n",
    "            hidden_s = hidden_states[hidden]\n",
    "            return self._pooled_handler(hidden_s, reduce_option)\n",
    "\n",
    "        elif hidden_concat:\n",
    "            last_states = [hidden_states[i] for i in hidden]\n",
    "            cat_hidden_states = torch.cat(tuple(last_states), dim=-1)\n",
    "            return torch.mean(cat_hidden_states, dim=1).squeeze()\n",
    "\n",
    "        last_states = [hidden_states[i] for i in hidden]\n",
    "        hidden_s = torch.cat(tuple(last_states), dim=1)\n",
    "\n",
    "        return self._pooled_handler(hidden_s, reduce_option)\n",
    "    \n",
    "    def create_matrix(self,\n",
    "                    content: List[str],\n",
    "                    hidden: Union[List[int], int] = -2,\n",
    "                    reduce_option: str = 'mean',\n",
    "                    hidden_concat: bool = False) -> ndarray:\n",
    "        \"\"\"\n",
    "        Create matrix from the embeddings.\n",
    "        :param content: The list of sentences.\n",
    "        :param hidden: Which hidden layer to use.\n",
    "        :param reduce_option: The reduce option to run.\n",
    "        :param hidden_concat: Whether or not to concat multiple hidden layers.\n",
    "        :return: A numpy array matrix of the given content.\n",
    "        \"\"\"\n",
    "        \n",
    "        return np.asarray([\n",
    "            np.squeeze(self.extract_embeddings(t, hidden=hidden, reduce_option=reduce_option, \n",
    "                                               hidden_concat=hidden_concat).data.cpu().numpy()) for t in content])\n",
    "    \n",
    "    def _pooled_handler(self, hidden: torch.Tensor,\n",
    "                        reduce_option: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Handles torch tensor.\n",
    "\n",
    "        :param hidden: The hidden torch tensor to process.\n",
    "        :param reduce_option: The reduce option to use, such as mean, etc.\n",
    "        :return: Returns a torch tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        if reduce_option == 'max':\n",
    "            return hidden.max(dim=1)[0].squeeze()\n",
    "\n",
    "        elif reduce_option == 'median':\n",
    "            return hidden.median(dim=1)[0].squeeze()\n",
    "\n",
    "        return hidden.mean(dim=1).squeeze()\n",
    "\n",
    "    def __call__(self,\n",
    "                content: List[str],\n",
    "                hidden: int = -2,\n",
    "                reduce_option: str = 'mean',\n",
    "                hidden_concat: bool = False) -> ndarray:\n",
    "        \"\"\"\n",
    "        Create matrix from the embeddings.\n",
    "\n",
    "        :param content: The list of sentences.\n",
    "        :param hidden: Which hidden layer to use.\n",
    "        :param reduce_option: The reduce option to run.\n",
    "        :param hidden_concat: Whether or not to concat multiple hidden layers.\n",
    "        :return: A numpy array matrix of the given content.\n",
    "        \"\"\"\n",
    "        return self.create_matrix(content, hidden, reduce_option, hidden_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T05:50:52.514938Z",
     "start_time": "2021-07-23T05:50:52.495989Z"
    },
    "code_folding": [
     0,
     16,
     38,
     52,
     66
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SentenceHandler(object):\n",
    "    def __init__(self, language=Vietnamese):\n",
    "        \"\"\"\n",
    "        Base Sentence Handler with Spacy support.\n",
    "        :param language: Determines the language to use with spacy.\n",
    "        \"\"\"\n",
    "        self.nlp = language()\n",
    "        try:\n",
    "            # Supports spacy 2.0\n",
    "            self.nlp.add_pipe(self.nlp.create_pipe('sentencizer'))\n",
    "            self.is_spacy_3 = False\n",
    "        except Exception:\n",
    "            # Supports spacy 3.0\n",
    "            self.nlp.add_pipe(\"sentencizer\")\n",
    "            self.is_spacy_3 = True\n",
    "\n",
    "    def sentence_processor(self, doc,\n",
    "                           min_length: int = 40,\n",
    "                           max_length: int = 600) -> List[str]:\n",
    "        \"\"\"\n",
    "        Processes a given spacy document and turns them into sentences.\n",
    "        :param doc: The document to use from spacy.\n",
    "        :param min_length: The minimum length a sentence should be to be considered.\n",
    "        :param max_length: The maximum length a sentence should be to be considered.\n",
    "        :return: Sentences.\n",
    "        \"\"\"\n",
    "        to_return = []\n",
    "\n",
    "        for c in doc.sents:\n",
    "            if max_length > len(c.text.strip()) > min_length:\n",
    "\n",
    "                if self.is_spacy_3:\n",
    "                    to_return.append(c.text.strip())\n",
    "                else:\n",
    "                    to_return.append(c.string.strip())\n",
    "\n",
    "        return to_return\n",
    "\n",
    "    def process(self, body: str,\n",
    "                min_length: int = 40,\n",
    "                max_length: int = 600) -> List[str]:\n",
    "        \"\"\"\n",
    "        Processes the content sentences.\n",
    "\n",
    "        :param body: The raw string body to process\n",
    "        :param min_length: Minimum length that the sentences must be\n",
    "        :param max_length: Max length that the sentences mus fall under\n",
    "        :return: Returns a list of sentences.\n",
    "        \"\"\"\n",
    "        doc = self.nlp(body)\n",
    "        return self.sentence_processor(doc, min_length, max_length)\n",
    "\n",
    "    def __call__(self, body: str,\n",
    "                 min_length: int = 40,\n",
    "                 max_length: int = 600) -> List[str]:\n",
    "        \"\"\"\n",
    "        Processes the content sentences.\n",
    "\n",
    "        :param body: The raw string body to process\n",
    "        :param min_length: Minimum length that the sentences must be\n",
    "        :param max_length: Max length that the sentences mus fall under\n",
    "        :return: Returns a list of sentences.\n",
    "        \"\"\"\n",
    "        return self.process(body, min_length, max_length)\n",
    "# removed previous import and related functionality since it's just a blank language model,\n",
    "# while neuralcoref requires passing pretrained language model via spacy.load() or spacy.lang.vi\n",
    "class CoreferenceHandler(SentenceHandler):\n",
    "    def __init__(self, spacy_model = Vietnamese,\n",
    "                 greedyness: float = 0.45):\n",
    "        \"\"\"\n",
    "        Corefence handler. Only works with spacy < 3.0.\n",
    "        :param spacy_model: The spacy model to use as default.\n",
    "        :param greedyness: The greedyness factor.\n",
    "        \"\"\"\n",
    "        self.nlp = spacy_model()\n",
    "            \n",
    "        self.nlp.add_pipe(self.nlp.create_pipe('sentencizer'))\n",
    "        neuralcoref.add_to_pipe(self.nlp, greedyness=greedyness)\n",
    "\n",
    "    def process(self, body: str, min_length: int = 40, max_length: int = 600):\n",
    "        \"\"\"\n",
    "        Processes the content sentences.\n",
    "        :param body: The raw string body to process\n",
    "        :param min_length: Minimum length that the sentences must be\n",
    "        :param max_length: Max length that the sentences mus fall under\n",
    "        :return: Returns a list of sentences.\n",
    "        \"\"\"\n",
    "        doc = self.nlp(body)._.coref_resolved\n",
    "        doc = self.nlp(doc)\n",
    "        return [c.string.strip()\n",
    "                for c in doc.sents\n",
    "                if max_length > len(c.string.strip()) > min_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T05:50:52.538874Z",
     "start_time": "2021-07-23T05:50:52.516933Z"
    },
    "code_folding": [
     0,
     4,
     22,
     32,
     42,
     65,
     87,
     99,
     127
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ClusterFeatures(object):\n",
    "    \"\"\"\n",
    "    Basic handling of clustering features.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                features: ndarray,\n",
    "                algorithm: str = 'kmeans',\n",
    "                pca_k: int = None,\n",
    "                random_state: int = 12345):\n",
    "        \"\"\"\n",
    "        :param features: the embedding matrix created by bert parent.\n",
    "        :param algorithm: Which clustering algorithm to use.\n",
    "        :param pca_k: If you want the features to be ran through pca, this is the components number.\n",
    "        :param random_state: Random state.\n",
    "        \"\"\"\n",
    "        if pca_k: self.features = PCA(n_components=pca_k).fit_transform(features)\n",
    "        else: self.features = features\n",
    "\n",
    "        self.algorithm = algorithm\n",
    "        self.pca_k = pca_k\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def __get_model(self, k: int):\n",
    "        \"\"\"\n",
    "        Retrieve clustering model.\n",
    "        :param k: amount of clusters.\n",
    "        :return: Clustering model.\n",
    "        \"\"\"\n",
    "        if self.algorithm == 'gmm':\n",
    "            return GaussianMixture(n_components=k, random_state=self.random_state)\n",
    "        return KMeans(n_clusters=k, random_state=self.random_state)\n",
    "\n",
    "    def __get_centroids(self, model):\n",
    "        \"\"\"\n",
    "        Retrieve centroids of model.\n",
    "        :param model: Clustering model.\n",
    "        :return: Centroids.\n",
    "        \"\"\"\n",
    "        if self.algorithm == 'gmm':\n",
    "            return model.means_\n",
    "        return model.cluster_centers_\n",
    "\n",
    "    def __find_closest_args(self, centroids: np.ndarray) -> Dict:\n",
    "        \"\"\"\n",
    "        Find the closest arguments to centroid.\n",
    "        :param centroids: Centroids to find closest.\n",
    "        :return: Closest arguments.\n",
    "        \"\"\"\n",
    "        centroid_min = 1e10\n",
    "        cur_arg = -1\n",
    "        args = {}\n",
    "        used_idx = []\n",
    "\n",
    "        for j, centroid in enumerate(centroids):\n",
    "            for i, feature in enumerate(self.features):\n",
    "                value = np.linalg.norm(feature - centroid)\n",
    "                if value < centroid_min and i not in used_idx:\n",
    "                    cur_arg = i\n",
    "                    centroid_min = value\n",
    "            used_idx.append(cur_arg)\n",
    "            args[j] = cur_arg\n",
    "            centroid_min = 1e10\n",
    "            cur_arg = -1\n",
    "        return args\n",
    "\n",
    "    def cluster(self, ratio: float = 0.1, num_sentences: int = None) -> List[int]:\n",
    "        \"\"\"\n",
    "        Clusters sentences based on the ratio.\n",
    "        :param ratio: Ratio to use for clustering.\n",
    "        :param num_sentences: Number of sentences. Overrides ratio.\n",
    "        :return: Sentences index that qualify for summary.\n",
    "        \"\"\"\n",
    "        if num_sentences is not None:\n",
    "            if num_sentences == 0: return []\n",
    "            k = min(num_sentences, len(self.features))\n",
    "        else:\n",
    "            k = max(int(len(self.features) * ratio), 1)\n",
    "        \n",
    "        #k = max(int(len(self.features) * ratio), 1)\n",
    "        model = self.__get_model(k).fit(self.features)\n",
    "\n",
    "        centroids = self.__get_centroids(model)\n",
    "        cluster_args = self.__find_closest_args(centroids)\n",
    "\n",
    "        sorted_values = sorted(cluster_args.values())\n",
    "        return sorted_values\n",
    "\n",
    "    def calculate_elbow(self, k_max: int) -> List[float]:\n",
    "        \"\"\"\n",
    "        Calculates elbow up to the provided k_max.\n",
    "        :param k_max: K_max to calculate elbow for.\n",
    "        :return: The inertias up to k_max.\n",
    "        \"\"\"\n",
    "        inertias = []\n",
    "        for k in range(1, min(k_max, len(self.features))):\n",
    "            model = self.__get_model(k).fit(self.features)\n",
    "            inertias.append(model.inertia_)\n",
    "        return inertias\n",
    "\n",
    "    def calculate_optimal_cluster(self, k_max: int):\n",
    "        \"\"\"\n",
    "        Calculates the optimal cluster based on Elbow.\n",
    "        :param k_max: The max k to search elbow for.\n",
    "        :return: The optimal cluster size.\n",
    "        \"\"\"\n",
    "        delta_1 = []\n",
    "        delta_2 = []\n",
    "\n",
    "        max_strength = 0\n",
    "        k = 1\n",
    "\n",
    "        inertias = self.calculate_elbow(k_max)\n",
    "\n",
    "        for i in range(len(inertias)):\n",
    "            delta_1.append(inertias[i] - inertias[i-1] if i > 0 else 0.0)\n",
    "            delta_2.append(delta_1[i] - delta_1[i-1] if i > 1 else 0.0)\n",
    "\n",
    "        for j in range(len(inertias)):\n",
    "            strength = 0 if j <= 1 or j == len(\n",
    "                inertias) - 1 else delta_2[j+1] - delta_1[j+1]\n",
    "\n",
    "            if strength > max_strength:\n",
    "                max_strength = strength\n",
    "                k = j + 1\n",
    "\n",
    "        return k\n",
    "\n",
    "    def __call__(self, ratio: float = 0.1, num_sentences: int = None) -> List[int]:\n",
    "        \"\"\"\n",
    "        Clusters sentences based on the ratio.\n",
    "        :param ratio: Ratio to use for clustering.\n",
    "        :param num_sentences: Number of sentences. Overrides ratio.\n",
    "        :return: Sentences index that qualify for summary.\n",
    "        \"\"\"\n",
    "        return self.cluster(ratio, num_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T05:50:52.895127Z",
     "start_time": "2021-07-23T05:50:52.541866Z"
    },
    "code_folding": [
     0,
     35,
     72,
     91,
     109,
     137,
     165,
     233,
     259,
     283,
     287
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ModelProcessor(object):\n",
    "    aggregate_map = {'mean': np.mean,\n",
    "                    'min': np.min,\n",
    "                    'median': np.median,\n",
    "                    'max': np.max}\n",
    "\n",
    "    def __init__(self,\n",
    "                model: str = 'bert-base-multilingual-uncased',\n",
    "                custom_model: PreTrainedModel = None,\n",
    "                custom_tokenizer: PreTrainedTokenizer = None,\n",
    "                hidden: Union[List[int], int] = -2,\n",
    "                reduce_option: str = 'mean',\n",
    "                sentence_handler: SentenceHandler = SentenceHandler(),\n",
    "                random_state: int = 12345,\n",
    "                hidden_concat: bool = False):\n",
    "        \"\"\"\n",
    "        This is the parent Bert Summarizer model. New methods should implement this class.\n",
    "        :param model: This parameter is associated with the inherit string parameters from the transformers library.\n",
    "        :param custom_model: If you have a pre-trained model, you can add the model class here.\n",
    "        :param custom_tokenizer: If you have a custom tokenizer, you can add the tokenizer here.\n",
    "        :param hidden: This signifies which layer(s) of the BERT model you would like to use as embeddings.\n",
    "        :param reduce_option: Given the output of the bert model, this param determines how you want to reduce results.\n",
    "        :param sentence_handler: The handler to process sentences. If want to use coreference, instantiate and pass.\n",
    "        CoreferenceHandler instance\n",
    "        :param random_state: The random state to reproduce summarizations.\n",
    "        :param hidden_concat: Whether or not to concat multiple hidden layers.\n",
    "        \"\"\"\n",
    "        np.random.seed(random_state)\n",
    "        self.model = BertParent(model, custom_model, custom_tokenizer)\n",
    "        self.hidden = hidden\n",
    "        self.reduce_option = reduce_option\n",
    "        self.sentence_handler = sentence_handler\n",
    "        self.random_state = random_state\n",
    "        self.hidden_concat = hidden_concat\n",
    "\n",
    "    def cluster_runner(self,\n",
    "        content: List[str],\n",
    "        ratio: float = 0.2,\n",
    "        algorithm: str = 'kmeans',\n",
    "        use_first: bool = True,\n",
    "        num_sentences: int = None) -> Tuple[List[str], np.ndarray]:\n",
    "        \"\"\"\n",
    "        Runs the cluster algorithm based on the hidden state. Returns both the embeddings and sentences.\n",
    "\n",
    "        :param content: Content list of sentences.\n",
    "        :param ratio: The ratio to use for clustering.\n",
    "        :param algorithm: Type of algorithm to use for clustering.\n",
    "        :param use_first: Return the first sentence in the output (helpful for news stories, etc).\n",
    "        :param num_sentences: Number of sentences to use for summarization.\n",
    "        :return: A tuple of summarized sentences and embeddings\n",
    "        \"\"\"\n",
    "        \n",
    "        if num_sentences is not None:\n",
    "            num_sentences = num_sentences if use_first else num_sentences\n",
    "\n",
    "        hidden = self.model(\n",
    "            content, self.hidden, self.reduce_option, hidden_concat=self.hidden_concat)\n",
    "        hidden_args = ClusterFeatures(\n",
    "            hidden, algorithm, random_state=self.random_state).cluster(ratio, num_sentences)\n",
    "        \n",
    "        if use_first:\n",
    "            if not hidden_args:\n",
    "                hidden_args.append(0)\n",
    "\n",
    "            elif hidden_args[0] != 0:\n",
    "                hidden_args.insert(0, 0)\n",
    "\n",
    "        sentences = [content[j] for j in hidden_args]\n",
    "        embeddings = np.asarray([hidden[j] for j in hidden_args])\n",
    "\n",
    "        return sentences, embeddings\n",
    "\n",
    "    def __run_clusters(self,\n",
    "                    content: List[str],\n",
    "                    ratio: float = 0.2,\n",
    "                    algorithm: str = 'kmeans',\n",
    "                    use_first: bool = True,\n",
    "                    num_sentences: int = None) -> List[str]:\n",
    "        \"\"\"\n",
    "        Runs clusters and returns sentences.\n",
    "        :param content: The content of sentences.\n",
    "        :param ratio: Ratio to use for for clustering.\n",
    "        :param algorithm: Algorithm selection for clustering.\n",
    "        :param use_first: Whether to use first sentence\n",
    "        :param num_sentences: Number of sentences. Overrides ratio.\n",
    "        :return: summarized sentences\n",
    "        \"\"\"\n",
    "        sentences, _ = self.cluster_runner(\n",
    "            content, ratio, algorithm, use_first, num_sentences)\n",
    "        return sentences\n",
    "\n",
    "    def __retrieve_summarized_embeddings(self,\n",
    "                                        content: List[str],\n",
    "                                        ratio: float = 0.2,\n",
    "                                        algorithm: str = 'kmeans',\n",
    "                                        use_first: bool = True,\n",
    "                                        num_sentences: int = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Retrieves embeddings of the summarized sentences.\n",
    "        :param content: The content of sentences.\n",
    "        :param ratio: Ratio to use for for clustering.\n",
    "        :param algorithm: Algorithm selection for clustering.\n",
    "        :param use_first: Whether to use first sentence\n",
    "        :return: Summarized embeddings\n",
    "        \"\"\"\n",
    "        _, embeddings = self.cluster_runner(\n",
    "            content, ratio, algorithm, use_first, num_sentences)\n",
    "        return embeddings\n",
    "\n",
    "    def calculate_elbow(self,\n",
    "                        body: str,\n",
    "                        algorithm: str = 'kmeans',\n",
    "                        min_length: int = 40,\n",
    "                        max_length: int = 600,\n",
    "                        k_max: int = None) -> List[float]:\n",
    "        \"\"\"\n",
    "        Calculates elbow across the clusters.\n",
    "\n",
    "        :param body: The input body to summarize.\n",
    "        :param algorithm: The algorithm to use for clustering.\n",
    "        :param min_length: The min length to use.\n",
    "        :param max_length: The max length to use.\n",
    "        :param k_max: The maximum number of clusters to search.\n",
    "        :return: List of elbow inertia values.\n",
    "        \"\"\"\n",
    "        sentences = self.sentence_handler(body, min_length, max_length)\n",
    "\n",
    "        if k_max is None:\n",
    "            k_max = len(sentences) - 1\n",
    "\n",
    "        hidden = self.model(sentences, self.hidden,\n",
    "                            self.reduce_option, hidden_concat=self.hidden_concat)\n",
    "        elbow = ClusterFeatures(\n",
    "            hidden, algorithm, random_state=self.random_state).calculate_elbow(k_max)\n",
    "\n",
    "        return elbow\n",
    "\n",
    "    def calculate_optimal_k(self,\n",
    "                            body: str,\n",
    "                            algorithm: str = 'kmeans',\n",
    "                            min_length: int = 40,\n",
    "                            max_length: int = 600,\n",
    "                            k_max: int = None):\n",
    "        \"\"\"\n",
    "        Calculates the optimal Elbow K.\n",
    "\n",
    "        :param body: The input body to summarize.\n",
    "        :param algorithm: The algorithm to use for clustering.\n",
    "        :param min_length: The min length to use.\n",
    "        :param max_length: The max length to use.\n",
    "        :param k_max: The maximum number of clusters to search.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sentences = self.sentence_handler(body, min_length, max_length)\n",
    "\n",
    "        if k_max is None:\n",
    "            k_max = len(sentences) - 1\n",
    "\n",
    "        hidden = self.model(sentences, self.hidden,\n",
    "                            self.reduce_option, hidden_concat=self.hidden_concat)\n",
    "        optimal_k = ClusterFeatures(\n",
    "            hidden, algorithm, random_state=self.random_state).calculate_optimal_cluster(k_max)\n",
    "\n",
    "        return optimal_k\n",
    "\n",
    "    def run_embeddings(self,\n",
    "                    body: str,\n",
    "                    ratio: float = 0.2,\n",
    "                    min_length: int = 40,\n",
    "                    max_length: int = 600,\n",
    "                    use_first: bool = True,\n",
    "                    algorithm: str = 'kmeans',\n",
    "                    num_sentences: int = None,\n",
    "                    aggregate: str = None) -> Optional[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Preprocesses the sentences, runs the clusters to find the centroids, then combines the embeddings.\n",
    "        :param body: The raw string body to process\n",
    "        :param ratio: Ratio of sentences to use\n",
    "        :param min_length: Minimum length of sentence candidates to utilize for the summary.\n",
    "        :param max_length: Maximum length of sentence candidates to utilize for the summary\n",
    "        :param use_first: Whether or not to use the first sentence\n",
    "        :param algorithm: Which clustering algorithm to use. (kmeans, gmm)\n",
    "        :param num_sentences: Number of sentences to use. Overrides ratio.\n",
    "        :param aggregate: One of mean, median, max, min. Applied on zero axis\n",
    "        :return: A summary embedding\n",
    "        \"\"\"\n",
    "        sentences = self.sentence_handler(body, min_length, max_length)\n",
    "\n",
    "        if sentences:\n",
    "            embeddings = self.__retrieve_summarized_embeddings(\n",
    "                sentences, ratio, algorithm, use_first, num_sentences)\n",
    "\n",
    "            if aggregate is not None:\n",
    "                assert aggregate in [\n",
    "                    'mean', 'median', 'max', 'min'], \"aggregate must be mean, min, max, or median\"\n",
    "                embeddings = self.aggregate_map[aggregate](embeddings, axis=0)\n",
    "\n",
    "            return embeddings\n",
    "\n",
    "        return None\n",
    "    \n",
    "    def run(self,\n",
    "            body: str,\n",
    "            ratio: float = 0.2,\n",
    "            min_length: int = 40,\n",
    "            max_length: int = 600,\n",
    "            use_first: bool = True,\n",
    "            algorithm: str = 'kmeans',\n",
    "            num_sentences: int = None,\n",
    "            return_as_list: bool = False) -> Union[List, str]:\n",
    "        \"\"\"\n",
    "        Preprocesses the sentences, runs the clusters to find the centroids, then combines the sentences.\n",
    "        :param body: The raw string body to process\n",
    "        :param ratio: Ratio of sentences to use\n",
    "        :param min_length: Minimum length of sentence candidates to utilize for the summary.\n",
    "        :param max_length: Maximum length of sentence candidates to utilize for the summary\n",
    "        :param use_first: Whether or not to use the first sentence\n",
    "        :param algorithm: Which clustering algorithm to use. (kmeans, gmm)\n",
    "        :param num_sentences: Number of sentences to use (overrides ratio).\n",
    "        :param return_as_list: Whether or not to return sentences as list.\n",
    "        :return: A summary sentence\n",
    "        \"\"\"\n",
    "        sentences = self.sentence_handler(body, min_length, max_length)\n",
    "\n",
    "        if sentences:\n",
    "            sentences = self.__run_clusters(sentences, ratio, algorithm, use_first, num_sentences)\n",
    "            \n",
    "        #Vietnamese spaCy language package also have _ characters between compound words, so we have to replace it\n",
    "        if return_as_list:\n",
    "            return sentences.replace('_', ' ')\n",
    "        else:\n",
    "            return ' '.join(sentences).replace('_',' ')\n",
    "\n",
    "    def __call__(self,\n",
    "                body: str,\n",
    "                ratio: float = 0.2,\n",
    "                min_length: int = 40,\n",
    "                max_length: int = 600,\n",
    "                use_first: bool = True,\n",
    "                algorithm: str = 'gmm', #kmeans\n",
    "                num_sentences: int = None,\n",
    "                return_as_list: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        (utility that wraps around the run function)\n",
    "        Preprocesses the sentences, runs the clusters to find the centroids, then combines the sentences.\n",
    "        :param body: The raw string body to process.\n",
    "        :param ratio: Ratio of sentences to use.\n",
    "        :param min_length: Minimum length of sentence candidates to utilize for the summary.\n",
    "        :param max_length: Maximum length of sentence candidates to utilize for the summary.\n",
    "        :param use_first: Whether or not to use the first sentence.\n",
    "        :param algorithm: Which clustering algorithm to use. (kmeans, gmm)\n",
    "        :param Number of sentences to use (overrides ratio).\n",
    "        :param return_as_list: Whether or not to return sentences as list.\n",
    "        :return: A summary sentence.\n",
    "        \"\"\"\n",
    "        return self.run(\n",
    "            body, ratio, min_length, max_length, algorithm=algorithm, use_first=use_first, num_sentences=num_sentences,\n",
    "            return_as_list=return_as_list)\n",
    "\n",
    "class Summarizer(ModelProcessor):\n",
    "    def __init__(self,\n",
    "        model: str = 'bert-base-multilingual-uncased',\n",
    "        custom_model: PreTrainedModel = None,\n",
    "        custom_tokenizer: PreTrainedTokenizer = None,\n",
    "        hidden: Union[List[int], int] = -2,\n",
    "        reduce_option: str = 'mean',\n",
    "        sentence_handler: SentenceHandler = SentenceHandler(),\n",
    "        random_state: int = 12345,\n",
    "        hidden_concat: bool = False):\n",
    "        \"\"\"\n",
    "        This is the main Bert Summarizer class.\n",
    "        :param model: This parameter is associated with the inherit string parameters from the transformers library.\n",
    "        :param custom_model: If you have a pre-trained model, you can add the model class here.\n",
    "        :param custom_tokenizer: If you have a custom tokenizer, you can add the tokenizer here.\n",
    "        :param hidden: This signifies which layer of the BERT model you would like to use as embeddings.\n",
    "        :param reduce_option: Given the output of the bert model, this param determines how you want to reduce results.\n",
    "        :param random_state: The random state to reproduce summarizations.\n",
    "        :param hidden_concat: Whether or not to concat multiple hidden layers.\n",
    "        \"\"\"\n",
    "        super(Summarizer, self).__init__(\n",
    "            model, custom_model, custom_tokenizer, hidden, reduce_option, sentence_handler, random_state, hidden_concat\n",
    "        )\n",
    "\n",
    "class TransformerSummarizer(ModelProcessor):\n",
    "    \"\"\"\n",
    "    Newer style that has keywords for models and tokenizers, but allows the user to change the type.\n",
    "    \"\"\"\n",
    "    MODEL_DICT = {\n",
    "                    'Bert': (BertModel, BertTokenizer),\n",
    "                    'GPT2': (GPT2Model, GPT2Tokenizer),\n",
    "                    'OpenAIGPT': (OpenAIGPTModel, OpenAIGPTTokenizer),\n",
    "                    'CTRL': (CTRLModel, CTRLTokenizer),\n",
    "                    'TransfoXL': (TransfoXLModel, TransfoXLTokenizer),\n",
    "                    'XLNet': (XLNetModel, XLNetTokenizer),\n",
    "                    'XLM': (XLMModel, XLMTokenizer),\n",
    "                    'DistilBert': (DistilBertModel, DistilBertTokenizer),\n",
    "                    'ALBERT': (AlbertModel, AlbertTokenizer),\n",
    "                    'phoBERT': (AutoModel, AutoTokenizer),\n",
    "    }\n",
    "    def __init__(self,\n",
    "                transformer_type: str = 'Bert',\n",
    "                transformer_model_key: str = 'bert-base-multilingual-uncased',\n",
    "                transformer_tokenizer_key: str = None,\n",
    "                hidden: Union[List[int], int] = -2,\n",
    "                reduce_option: str = 'mean',\n",
    "                sentence_handler: SentenceHandler = SentenceHandler(),\n",
    "                random_state: int = 12345,\n",
    "                hidden_concat: bool = False):\n",
    "        \"\"\"\n",
    "        :param transformer_type: The Transformer type, such as Bert, GPT2, DistilBert, etc.\n",
    "        :param transformer_model_key: The transformer model key. This is the directory for the model.\n",
    "        :param transformer_tokenizer_key: The transformer tokenizer key. This is the tokenizer directory.\n",
    "        :param hidden: The hidden output layers to use for the summarization.\n",
    "        :param reduce_option: The reduce option, such as mean, max, min, median, etc.\n",
    "        :param sentence_handler: The sentence handler class to process the raw text.\n",
    "        :param random_state: The random state to use.\n",
    "        :param hidden_concat: Deprecated hidden concat option.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.MODEL_DICT['Roberta'] = (RobertaModel, RobertaTokenizer)\n",
    "            self.MODEL_DICT['Albert'] = (AlbertModel, AlbertTokenizer)\n",
    "            self.MODEL_DICT['Camembert'] = (CamembertModel, CamembertTokenizer)\n",
    "            self.MODEL_DICT['Bart'] = (BartModel, BartTokenizer)\n",
    "            self.MODEL_DICT['Longformer'] = (LongformerModel, LongformerTokenizer)\n",
    "        except Exception:\n",
    "            pass  # older transformer version\n",
    "\n",
    "        model_clz, tokenizer_clz = self.MODEL_DICT[transformer_type]\n",
    "        model = model_clz.from_pretrained(\n",
    "            transformer_model_key, output_hidden_states=True)\n",
    "\n",
    "        tokenizer = tokenizer_clz.from_pretrained(\n",
    "            transformer_tokenizer_key if transformer_tokenizer_key is not None else transformer_model_key\n",
    "        )\n",
    "\n",
    "        super().__init__(\n",
    "            None, model, tokenizer, hidden, reduce_option, sentence_handler, random_state, hidden_concat\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T05:50:52.902934Z",
     "start_time": "2021-07-23T05:50:52.897932Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def rouge_dist(hypotheses, references):\n",
    "    rouge = PyRouge(rouge_n=(1, 2), rouge_l=True, rouge_su=True, skip_gap=4)\n",
    "    scores = rouge.evaluate(hypotheses, references)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T05:50:52.916908Z",
     "start_time": "2021-07-23T05:50:52.905900Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def loadtxt(path, ref=False):\n",
    "    bodyinput=[]\n",
    "    # Change the directory\n",
    "    os.chdir(path)\n",
    "    # Read text File  \n",
    "    def read_text_file(file_path):\n",
    "        with open(file_path, 'r', encoding = 'utf-8') as f:\n",
    "            ftext = f.read()\n",
    "            ftext = ftext.replace('\\ufeff ','').replace('\\ufeff','').replace('\\n','')\n",
    "            return ftext\n",
    "        \n",
    "    # iterate through all file\n",
    "    for file in os.listdir():\n",
    "        if file.endswith(\".txt\"):\n",
    "            file_path = f\"{path}\\{file}\"\n",
    "            if ref:\n",
    "                bodyinput.append([read_text_file(file_path)])\n",
    "            else: \n",
    "                bodyinput.append(read_text_file(file_path))\n",
    "    return bodyinput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Analyzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T05:50:53.250984Z",
     "start_time": "2021-07-23T05:50:53.245005Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "                'bert-base-multilingual-uncased': (BertModel, BertTokenizer),\n",
    "                'bert-large-uncased': (BertModel, BertTokenizer),\n",
    "                'gpt2': (GPT2Model, GPT2Tokenizer),\n",
    "                'facebook/bart-large': (BartModel, BartTokenizer),\n",
    "                'openai-gpt': (OpenAIGPTModel, OpenAIGPTTokenizer),\n",
    "                'ctrl': (CTRLModel, CTRLTokenizer),\n",
    "                'transfo-xl-wt103': (TransfoXLModel, TransfoXLTokenizer), \n",
    "                'xlnet-large-cased': (XLNetModel, XLNetTokenizer),\n",
    "                'xlm-mlm-enfr-1024': (XLMModel, XLMTokenizer),\n",
    "                'distilbert-base-uncased': (DistilBertModel, DistilBertTokenizer),\n",
    "                'albert-large-v2': (AlbertModel, AlbertTokenizer),\n",
    "                'allenai/scibert_scivocab_uncased': (AutoModel, AutoTokenizer),\n",
    "                'vinai/phobert-large': (AutoModel, AutoTokenizer)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T05:50:53.302846Z",
     "start_time": "2021-07-23T05:50:53.297858Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "category = ['boKHCN', 'Chinh Tri', 'khoahoc_giaoduc', 'kinhte', 'Van Hoa', 'Xa Hoi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-21T14:11:29.299015Z",
     "start_time": "2021-07-21T14:11:28.883358Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "idx = category[0]\n",
    "pathbody = 'E:/TextSummarization/donvanban/Plaintext/' + idx \n",
    "pathref = 'E:/TextSummarization/donvanban/Summary_manual/' + idx \n",
    "rootbody = loadtxt(pathbody)\n",
    "refbody = loadtxt(pathref, ref=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-21T15:07:04.685217Z",
     "start_time": "2021-07-21T14:11:33.662318Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
      "c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ..\\aten\\src\\ATen\\native\\BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      "c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\transformers\\configuration_transfo_xl.py:146: FutureWarning: The config parameter `tie_weight` is deprecated. Please use `tie_word_embeddings` instead.\n",
      "  FutureWarning,\n",
      "c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\transformers\\configuration_xlnet.py:212: FutureWarning: This config doesn't use attention memories, a core feature of XLNet. Consider setting `men_len` to a non-zero value, for example `xlnet = XLNetLMHeadModel.from_pretrained('xlnet-base-cased'', mem_len=1024)`, for accurate training performance as well as an order of magnitude faster inference. Starting from version 3.5.0, the default parameter will be 1024, following the implementation in https://arxiv.org/abs/1906.08237\n",
      "  FutureWarning,\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "res = {'Rouge1_p':[], 'Rouge2_p':[], 'RougeL_p':[], 'RougeSU4_p': [],\n",
    "        'Rouge1_r':[], 'Rouge2_r':[], 'RougeL_r':[], 'RougeSU4_r': [],\n",
    "        'Rouge1_f':[], 'Rouge2_f':[], 'RougeL_f':[], 'RougeSU4_f': [],\n",
    "      }\n",
    "\n",
    "for idx in model_dict.keys():    \n",
    "    model = Summarizer(model = idx, sentence_handler=CoreferenceHandler())\n",
    "    all_result = []\n",
    "    for jdx in range(len(rootbody)):\n",
    "        result = model(rootbody[jdx], ratio=2*len(refbody[jdx])/len(rootbody[jdx]))\n",
    "        all_result.append(result)\n",
    "    r_rouge = rouge_dist(all_result, refbody)\n",
    "    for ind in ['p', 'r', 'f']:\n",
    "        res['Rouge1_'+ind].append(r_rouge['rouge-1'][ind])\n",
    "        res['Rouge2_'+ind].append(r_rouge['rouge-2'][ind])\n",
    "        res['RougeL_'+ind].append(r_rouge['rouge-l'][ind])\n",
    "        res['RougeSU4_'+ind].append(r_rouge['rouge-su4'][ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-21T15:07:21.238799Z",
     "start_time": "2021-07-21T15:07:21.176706Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rouge1_p</th>\n",
       "      <th>Rouge2_p</th>\n",
       "      <th>RougeL_p</th>\n",
       "      <th>RougeSU4_p</th>\n",
       "      <th>Rouge1_r</th>\n",
       "      <th>Rouge2_r</th>\n",
       "      <th>RougeL_r</th>\n",
       "      <th>RougeSU4_r</th>\n",
       "      <th>Rouge1_f</th>\n",
       "      <th>Rouge2_f</th>\n",
       "      <th>RougeL_f</th>\n",
       "      <th>RougeSU4_f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bert-base-multilingual-uncased</th>\n",
       "      <td>0.589949</td>\n",
       "      <td>0.468815</td>\n",
       "      <td>0.471425</td>\n",
       "      <td>0.440070</td>\n",
       "      <td>0.531650</td>\n",
       "      <td>0.419202</td>\n",
       "      <td>0.422021</td>\n",
       "      <td>0.392686</td>\n",
       "      <td>0.559284</td>\n",
       "      <td>0.442622</td>\n",
       "      <td>0.445357</td>\n",
       "      <td>0.415030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert-large-uncased</th>\n",
       "      <td>0.577423</td>\n",
       "      <td>0.442054</td>\n",
       "      <td>0.440485</td>\n",
       "      <td>0.413317</td>\n",
       "      <td>0.443022</td>\n",
       "      <td>0.337761</td>\n",
       "      <td>0.336407</td>\n",
       "      <td>0.315294</td>\n",
       "      <td>0.501372</td>\n",
       "      <td>0.382934</td>\n",
       "      <td>0.381475</td>\n",
       "      <td>0.357712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2</th>\n",
       "      <td>0.592837</td>\n",
       "      <td>0.456343</td>\n",
       "      <td>0.459239</td>\n",
       "      <td>0.423710</td>\n",
       "      <td>0.477464</td>\n",
       "      <td>0.366082</td>\n",
       "      <td>0.367362</td>\n",
       "      <td>0.339460</td>\n",
       "      <td>0.528932</td>\n",
       "      <td>0.406259</td>\n",
       "      <td>0.408194</td>\n",
       "      <td>0.376934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>facebook/bart-large</th>\n",
       "      <td>0.559149</td>\n",
       "      <td>0.418020</td>\n",
       "      <td>0.430839</td>\n",
       "      <td>0.388407</td>\n",
       "      <td>0.472508</td>\n",
       "      <td>0.358028</td>\n",
       "      <td>0.366000</td>\n",
       "      <td>0.333033</td>\n",
       "      <td>0.512190</td>\n",
       "      <td>0.385705</td>\n",
       "      <td>0.395781</td>\n",
       "      <td>0.358595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openai-gpt</th>\n",
       "      <td>0.572291</td>\n",
       "      <td>0.433563</td>\n",
       "      <td>0.442683</td>\n",
       "      <td>0.404568</td>\n",
       "      <td>0.466779</td>\n",
       "      <td>0.355695</td>\n",
       "      <td>0.361708</td>\n",
       "      <td>0.331607</td>\n",
       "      <td>0.514178</td>\n",
       "      <td>0.390787</td>\n",
       "      <td>0.398120</td>\n",
       "      <td>0.364472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ctrl</th>\n",
       "      <td>0.572195</td>\n",
       "      <td>0.439382</td>\n",
       "      <td>0.430547</td>\n",
       "      <td>0.413148</td>\n",
       "      <td>0.516807</td>\n",
       "      <td>0.388970</td>\n",
       "      <td>0.382233</td>\n",
       "      <td>0.363626</td>\n",
       "      <td>0.543093</td>\n",
       "      <td>0.412642</td>\n",
       "      <td>0.404954</td>\n",
       "      <td>0.386809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transfo-xl-wt103</th>\n",
       "      <td>0.599472</td>\n",
       "      <td>0.454298</td>\n",
       "      <td>0.446087</td>\n",
       "      <td>0.423647</td>\n",
       "      <td>0.411946</td>\n",
       "      <td>0.304241</td>\n",
       "      <td>0.302832</td>\n",
       "      <td>0.282794</td>\n",
       "      <td>0.488324</td>\n",
       "      <td>0.364427</td>\n",
       "      <td>0.360758</td>\n",
       "      <td>0.339179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xlnet-large-cased</th>\n",
       "      <td>0.585887</td>\n",
       "      <td>0.463378</td>\n",
       "      <td>0.466526</td>\n",
       "      <td>0.435221</td>\n",
       "      <td>0.532594</td>\n",
       "      <td>0.418054</td>\n",
       "      <td>0.419548</td>\n",
       "      <td>0.392126</td>\n",
       "      <td>0.557971</td>\n",
       "      <td>0.439551</td>\n",
       "      <td>0.441791</td>\n",
       "      <td>0.412551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xlm-mlm-enfr-1024</th>\n",
       "      <td>0.573055</td>\n",
       "      <td>0.437195</td>\n",
       "      <td>0.430924</td>\n",
       "      <td>0.407219</td>\n",
       "      <td>0.425831</td>\n",
       "      <td>0.316865</td>\n",
       "      <td>0.318507</td>\n",
       "      <td>0.294447</td>\n",
       "      <td>0.488594</td>\n",
       "      <td>0.367429</td>\n",
       "      <td>0.366284</td>\n",
       "      <td>0.341771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distilbert-base-uncased</th>\n",
       "      <td>0.562064</td>\n",
       "      <td>0.423952</td>\n",
       "      <td>0.431258</td>\n",
       "      <td>0.395780</td>\n",
       "      <td>0.495183</td>\n",
       "      <td>0.379124</td>\n",
       "      <td>0.383464</td>\n",
       "      <td>0.354335</td>\n",
       "      <td>0.526508</td>\n",
       "      <td>0.400287</td>\n",
       "      <td>0.405959</td>\n",
       "      <td>0.373912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>albert-large-v2</th>\n",
       "      <td>0.576495</td>\n",
       "      <td>0.444851</td>\n",
       "      <td>0.446179</td>\n",
       "      <td>0.418132</td>\n",
       "      <td>0.461067</td>\n",
       "      <td>0.356349</td>\n",
       "      <td>0.357276</td>\n",
       "      <td>0.334547</td>\n",
       "      <td>0.512360</td>\n",
       "      <td>0.395712</td>\n",
       "      <td>0.396809</td>\n",
       "      <td>0.371699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allenai/scibert_scivocab_uncased</th>\n",
       "      <td>0.556842</td>\n",
       "      <td>0.418162</td>\n",
       "      <td>0.424624</td>\n",
       "      <td>0.389233</td>\n",
       "      <td>0.486641</td>\n",
       "      <td>0.366889</td>\n",
       "      <td>0.370613</td>\n",
       "      <td>0.342096</td>\n",
       "      <td>0.519380</td>\n",
       "      <td>0.390851</td>\n",
       "      <td>0.395784</td>\n",
       "      <td>0.364146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vinai/phobert-large</th>\n",
       "      <td>0.556205</td>\n",
       "      <td>0.432335</td>\n",
       "      <td>0.430730</td>\n",
       "      <td>0.401390</td>\n",
       "      <td>0.526337</td>\n",
       "      <td>0.409172</td>\n",
       "      <td>0.406588</td>\n",
       "      <td>0.379919</td>\n",
       "      <td>0.540859</td>\n",
       "      <td>0.420435</td>\n",
       "      <td>0.418311</td>\n",
       "      <td>0.390360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Rouge1_p  Rouge2_p  RougeL_p  RougeSU4_p  \\\n",
       "bert-base-multilingual-uncased    0.589949  0.468815  0.471425    0.440070   \n",
       "bert-large-uncased                0.577423  0.442054  0.440485    0.413317   \n",
       "gpt2                              0.592837  0.456343  0.459239    0.423710   \n",
       "facebook/bart-large               0.559149  0.418020  0.430839    0.388407   \n",
       "openai-gpt                        0.572291  0.433563  0.442683    0.404568   \n",
       "ctrl                              0.572195  0.439382  0.430547    0.413148   \n",
       "transfo-xl-wt103                  0.599472  0.454298  0.446087    0.423647   \n",
       "xlnet-large-cased                 0.585887  0.463378  0.466526    0.435221   \n",
       "xlm-mlm-enfr-1024                 0.573055  0.437195  0.430924    0.407219   \n",
       "distilbert-base-uncased           0.562064  0.423952  0.431258    0.395780   \n",
       "albert-large-v2                   0.576495  0.444851  0.446179    0.418132   \n",
       "allenai/scibert_scivocab_uncased  0.556842  0.418162  0.424624    0.389233   \n",
       "vinai/phobert-large               0.556205  0.432335  0.430730    0.401390   \n",
       "\n",
       "                                  Rouge1_r  Rouge2_r  RougeL_r  RougeSU4_r  \\\n",
       "bert-base-multilingual-uncased    0.531650  0.419202  0.422021    0.392686   \n",
       "bert-large-uncased                0.443022  0.337761  0.336407    0.315294   \n",
       "gpt2                              0.477464  0.366082  0.367362    0.339460   \n",
       "facebook/bart-large               0.472508  0.358028  0.366000    0.333033   \n",
       "openai-gpt                        0.466779  0.355695  0.361708    0.331607   \n",
       "ctrl                              0.516807  0.388970  0.382233    0.363626   \n",
       "transfo-xl-wt103                  0.411946  0.304241  0.302832    0.282794   \n",
       "xlnet-large-cased                 0.532594  0.418054  0.419548    0.392126   \n",
       "xlm-mlm-enfr-1024                 0.425831  0.316865  0.318507    0.294447   \n",
       "distilbert-base-uncased           0.495183  0.379124  0.383464    0.354335   \n",
       "albert-large-v2                   0.461067  0.356349  0.357276    0.334547   \n",
       "allenai/scibert_scivocab_uncased  0.486641  0.366889  0.370613    0.342096   \n",
       "vinai/phobert-large               0.526337  0.409172  0.406588    0.379919   \n",
       "\n",
       "                                  Rouge1_f  Rouge2_f  RougeL_f  RougeSU4_f  \n",
       "bert-base-multilingual-uncased    0.559284  0.442622  0.445357    0.415030  \n",
       "bert-large-uncased                0.501372  0.382934  0.381475    0.357712  \n",
       "gpt2                              0.528932  0.406259  0.408194    0.376934  \n",
       "facebook/bart-large               0.512190  0.385705  0.395781    0.358595  \n",
       "openai-gpt                        0.514178  0.390787  0.398120    0.364472  \n",
       "ctrl                              0.543093  0.412642  0.404954    0.386809  \n",
       "transfo-xl-wt103                  0.488324  0.364427  0.360758    0.339179  \n",
       "xlnet-large-cased                 0.557971  0.439551  0.441791    0.412551  \n",
       "xlm-mlm-enfr-1024                 0.488594  0.367429  0.366284    0.341771  \n",
       "distilbert-base-uncased           0.526508  0.400287  0.405959    0.373912  \n",
       "albert-large-v2                   0.512360  0.395712  0.396809    0.371699  \n",
       "allenai/scibert_scivocab_uncased  0.519380  0.390851  0.395784    0.364146  \n",
       "vinai/phobert-large               0.540859  0.420435  0.418311    0.390360  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#score f, precision p, recall r\n",
    "df = pd.DataFrame(res, index = model_dict.keys())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Analyzing others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T03:54:49.355952Z",
     "start_time": "2021-07-23T03:54:48.443216Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XH01.txt\n",
      "XH02.txt\n",
      "XH03.txt\n",
      "XH04.txt\n",
      "XH05.txt\n",
      "XH06.txt\n",
      "XH07.txt\n",
      "XH08.txt\n",
      "XH09.txt\n",
      "XH10.txt\n",
      "XH11.txt\n",
      "XH12.txt\n",
      "XH13.txt\n",
      "XH14.txt\n",
      "XH15.txt\n",
      "XH16.txt\n",
      "XH17.txt\n",
      "XH18.txt\n",
      "XH19.txt\n",
      "XH20.txt\n",
      "XH21.txt\n",
      "XH22.txt\n",
      "XH23.txt\n",
      "XH24.txt\n",
      "XH25.txt\n",
      "XH26.txt\n",
      "XH27.txt\n",
      "XH28.txt\n",
      "XH29.txt\n",
      "XH30.txt\n",
      "XH31.txt\n",
      "XH32.txt\n",
      "XH33.txt\n",
      "XH34.txt\n",
      "XH35.txt\n"
     ]
    }
   ],
   "source": [
    "#check utf-8 encode errors on datasets\n",
    "#plain vh020304, summary ct17, vh11\n",
    "'''\n",
    "os.chdir('E:/TextSummarization/donvanban/Summary_manual/Xa Hoi')\n",
    "for file in os.listdir():\n",
    "        print(file)\n",
    "        if file.endswith(\".txt\"):\n",
    "            file_path = f\"{'E:/TextSummarization/donvanban/Summary_manual/Xa Hoi'}\\{file}\"\n",
    "            with open(file_path, 'r', encoding = 'utf-8') as f:\n",
    "                ftext = f.read()\n",
    "                ftext = ftext.replace('\\ufeff ','').replace('\\ufeff','').replace('\\n','')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T07:14:24.485103Z",
     "start_time": "2021-07-22T06:13:40.455849Z"
    },
    "code_folding": [
     8,
     13
    ],
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
      "c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ..\\aten\\src\\ATen\\native\\BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      "c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\transformers\\configuration_transfo_xl.py:146: FutureWarning: The config parameter `tie_weight` is deprecated. Please use `tie_word_embeddings` instead.\n",
      "  FutureWarning,\n",
      "c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\transformers\\configuration_xlnet.py:212: FutureWarning: This config doesn't use attention memories, a core feature of XLNet. Consider setting `men_len` to a non-zero value, for example `xlnet = XLNetLMHeadModel.from_pretrained('xlnet-base-cased'', mem_len=1024)`, for accurate training performance as well as an order of magnitude faster inference. Starting from version 3.5.0, the default parameter will be 1024, following the implementation in https://arxiv.org/abs/1906.08237\n",
      "  FutureWarning,\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rouge1_p</th>\n",
       "      <th>Rouge2_p</th>\n",
       "      <th>RougeL_p</th>\n",
       "      <th>RougeSU4_p</th>\n",
       "      <th>Rouge1_r</th>\n",
       "      <th>Rouge2_r</th>\n",
       "      <th>RougeL_r</th>\n",
       "      <th>RougeSU4_r</th>\n",
       "      <th>Rouge1_f</th>\n",
       "      <th>Rouge2_f</th>\n",
       "      <th>RougeL_f</th>\n",
       "      <th>RougeSU4_f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bert-base-multilingual-uncased</th>\n",
       "      <td>0.606898</td>\n",
       "      <td>0.488426</td>\n",
       "      <td>0.511353</td>\n",
       "      <td>0.464321</td>\n",
       "      <td>0.460991</td>\n",
       "      <td>0.372269</td>\n",
       "      <td>0.387233</td>\n",
       "      <td>0.352674</td>\n",
       "      <td>0.523977</td>\n",
       "      <td>0.422509</td>\n",
       "      <td>0.440721</td>\n",
       "      <td>0.400868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert-large-uncased</th>\n",
       "      <td>0.584729</td>\n",
       "      <td>0.466408</td>\n",
       "      <td>0.493596</td>\n",
       "      <td>0.447245</td>\n",
       "      <td>0.423522</td>\n",
       "      <td>0.342373</td>\n",
       "      <td>0.357176</td>\n",
       "      <td>0.327926</td>\n",
       "      <td>0.491238</td>\n",
       "      <td>0.394879</td>\n",
       "      <td>0.414449</td>\n",
       "      <td>0.378403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2</th>\n",
       "      <td>0.591884</td>\n",
       "      <td>0.470366</td>\n",
       "      <td>0.495039</td>\n",
       "      <td>0.446783</td>\n",
       "      <td>0.428530</td>\n",
       "      <td>0.347310</td>\n",
       "      <td>0.361333</td>\n",
       "      <td>0.329626</td>\n",
       "      <td>0.497131</td>\n",
       "      <td>0.399578</td>\n",
       "      <td>0.417748</td>\n",
       "      <td>0.379365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>facebook/bart-large</th>\n",
       "      <td>0.571642</td>\n",
       "      <td>0.449727</td>\n",
       "      <td>0.474903</td>\n",
       "      <td>0.430690</td>\n",
       "      <td>0.441181</td>\n",
       "      <td>0.354861</td>\n",
       "      <td>0.371311</td>\n",
       "      <td>0.339761</td>\n",
       "      <td>0.498009</td>\n",
       "      <td>0.396701</td>\n",
       "      <td>0.416766</td>\n",
       "      <td>0.379860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openai-gpt</th>\n",
       "      <td>0.603978</td>\n",
       "      <td>0.479673</td>\n",
       "      <td>0.500607</td>\n",
       "      <td>0.456440</td>\n",
       "      <td>0.413490</td>\n",
       "      <td>0.329500</td>\n",
       "      <td>0.342801</td>\n",
       "      <td>0.312312</td>\n",
       "      <td>0.490903</td>\n",
       "      <td>0.390652</td>\n",
       "      <td>0.406940</td>\n",
       "      <td>0.370865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ctrl</th>\n",
       "      <td>0.570475</td>\n",
       "      <td>0.447075</td>\n",
       "      <td>0.464067</td>\n",
       "      <td>0.426458</td>\n",
       "      <td>0.449808</td>\n",
       "      <td>0.346983</td>\n",
       "      <td>0.360848</td>\n",
       "      <td>0.329545</td>\n",
       "      <td>0.503006</td>\n",
       "      <td>0.390720</td>\n",
       "      <td>0.406000</td>\n",
       "      <td>0.371790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transfo-xl-wt103</th>\n",
       "      <td>0.637110</td>\n",
       "      <td>0.514270</td>\n",
       "      <td>0.536534</td>\n",
       "      <td>0.490145</td>\n",
       "      <td>0.418280</td>\n",
       "      <td>0.342160</td>\n",
       "      <td>0.355364</td>\n",
       "      <td>0.324859</td>\n",
       "      <td>0.505008</td>\n",
       "      <td>0.410921</td>\n",
       "      <td>0.427549</td>\n",
       "      <td>0.390742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xlnet-large-cased</th>\n",
       "      <td>0.581520</td>\n",
       "      <td>0.467159</td>\n",
       "      <td>0.483462</td>\n",
       "      <td>0.448211</td>\n",
       "      <td>0.457777</td>\n",
       "      <td>0.366858</td>\n",
       "      <td>0.380432</td>\n",
       "      <td>0.350807</td>\n",
       "      <td>0.512282</td>\n",
       "      <td>0.410977</td>\n",
       "      <td>0.425803</td>\n",
       "      <td>0.393572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xlm-mlm-enfr-1024</th>\n",
       "      <td>0.581030</td>\n",
       "      <td>0.458219</td>\n",
       "      <td>0.480850</td>\n",
       "      <td>0.434891</td>\n",
       "      <td>0.437265</td>\n",
       "      <td>0.350573</td>\n",
       "      <td>0.363914</td>\n",
       "      <td>0.333355</td>\n",
       "      <td>0.498999</td>\n",
       "      <td>0.397232</td>\n",
       "      <td>0.414288</td>\n",
       "      <td>0.377413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distilbert-base-uncased</th>\n",
       "      <td>0.613016</td>\n",
       "      <td>0.491844</td>\n",
       "      <td>0.519021</td>\n",
       "      <td>0.472632</td>\n",
       "      <td>0.437599</td>\n",
       "      <td>0.359126</td>\n",
       "      <td>0.378010</td>\n",
       "      <td>0.345194</td>\n",
       "      <td>0.510664</td>\n",
       "      <td>0.415135</td>\n",
       "      <td>0.437432</td>\n",
       "      <td>0.398984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>albert-large-v2</th>\n",
       "      <td>0.627881</td>\n",
       "      <td>0.513256</td>\n",
       "      <td>0.519786</td>\n",
       "      <td>0.489416</td>\n",
       "      <td>0.408149</td>\n",
       "      <td>0.330163</td>\n",
       "      <td>0.336601</td>\n",
       "      <td>0.312926</td>\n",
       "      <td>0.494713</td>\n",
       "      <td>0.401836</td>\n",
       "      <td>0.408601</td>\n",
       "      <td>0.381760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allenai/scibert_scivocab_uncased</th>\n",
       "      <td>0.593624</td>\n",
       "      <td>0.473396</td>\n",
       "      <td>0.494526</td>\n",
       "      <td>0.450434</td>\n",
       "      <td>0.431407</td>\n",
       "      <td>0.345968</td>\n",
       "      <td>0.359651</td>\n",
       "      <td>0.329073</td>\n",
       "      <td>0.499680</td>\n",
       "      <td>0.399773</td>\n",
       "      <td>0.416440</td>\n",
       "      <td>0.380307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vinai/phobert-large</th>\n",
       "      <td>0.550682</td>\n",
       "      <td>0.440384</td>\n",
       "      <td>0.456234</td>\n",
       "      <td>0.417305</td>\n",
       "      <td>0.446242</td>\n",
       "      <td>0.354763</td>\n",
       "      <td>0.368099</td>\n",
       "      <td>0.336358</td>\n",
       "      <td>0.492991</td>\n",
       "      <td>0.392964</td>\n",
       "      <td>0.407455</td>\n",
       "      <td>0.372484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Rouge1_p  Rouge2_p  RougeL_p  RougeSU4_p  \\\n",
       "bert-base-multilingual-uncased    0.606898  0.488426  0.511353    0.464321   \n",
       "bert-large-uncased                0.584729  0.466408  0.493596    0.447245   \n",
       "gpt2                              0.591884  0.470366  0.495039    0.446783   \n",
       "facebook/bart-large               0.571642  0.449727  0.474903    0.430690   \n",
       "openai-gpt                        0.603978  0.479673  0.500607    0.456440   \n",
       "ctrl                              0.570475  0.447075  0.464067    0.426458   \n",
       "transfo-xl-wt103                  0.637110  0.514270  0.536534    0.490145   \n",
       "xlnet-large-cased                 0.581520  0.467159  0.483462    0.448211   \n",
       "xlm-mlm-enfr-1024                 0.581030  0.458219  0.480850    0.434891   \n",
       "distilbert-base-uncased           0.613016  0.491844  0.519021    0.472632   \n",
       "albert-large-v2                   0.627881  0.513256  0.519786    0.489416   \n",
       "allenai/scibert_scivocab_uncased  0.593624  0.473396  0.494526    0.450434   \n",
       "vinai/phobert-large               0.550682  0.440384  0.456234    0.417305   \n",
       "\n",
       "                                  Rouge1_r  Rouge2_r  RougeL_r  RougeSU4_r  \\\n",
       "bert-base-multilingual-uncased    0.460991  0.372269  0.387233    0.352674   \n",
       "bert-large-uncased                0.423522  0.342373  0.357176    0.327926   \n",
       "gpt2                              0.428530  0.347310  0.361333    0.329626   \n",
       "facebook/bart-large               0.441181  0.354861  0.371311    0.339761   \n",
       "openai-gpt                        0.413490  0.329500  0.342801    0.312312   \n",
       "ctrl                              0.449808  0.346983  0.360848    0.329545   \n",
       "transfo-xl-wt103                  0.418280  0.342160  0.355364    0.324859   \n",
       "xlnet-large-cased                 0.457777  0.366858  0.380432    0.350807   \n",
       "xlm-mlm-enfr-1024                 0.437265  0.350573  0.363914    0.333355   \n",
       "distilbert-base-uncased           0.437599  0.359126  0.378010    0.345194   \n",
       "albert-large-v2                   0.408149  0.330163  0.336601    0.312926   \n",
       "allenai/scibert_scivocab_uncased  0.431407  0.345968  0.359651    0.329073   \n",
       "vinai/phobert-large               0.446242  0.354763  0.368099    0.336358   \n",
       "\n",
       "                                  Rouge1_f  Rouge2_f  RougeL_f  RougeSU4_f  \n",
       "bert-base-multilingual-uncased    0.523977  0.422509  0.440721    0.400868  \n",
       "bert-large-uncased                0.491238  0.394879  0.414449    0.378403  \n",
       "gpt2                              0.497131  0.399578  0.417748    0.379365  \n",
       "facebook/bart-large               0.498009  0.396701  0.416766    0.379860  \n",
       "openai-gpt                        0.490903  0.390652  0.406940    0.370865  \n",
       "ctrl                              0.503006  0.390720  0.406000    0.371790  \n",
       "transfo-xl-wt103                  0.505008  0.410921  0.427549    0.390742  \n",
       "xlnet-large-cased                 0.512282  0.410977  0.425803    0.393572  \n",
       "xlm-mlm-enfr-1024                 0.498999  0.397232  0.414288    0.377413  \n",
       "distilbert-base-uncased           0.510664  0.415135  0.437432    0.398984  \n",
       "albert-large-v2                   0.494713  0.401836  0.408601    0.381760  \n",
       "allenai/scibert_scivocab_uncased  0.499680  0.399773  0.416440    0.380307  \n",
       "vinai/phobert-large               0.492991  0.392964  0.407455    0.372484  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chinh tri\n",
    "idx = category[1]\n",
    "\n",
    "pathbody = 'E:/TextSummarization/donvanban/Plaintext/' + idx \n",
    "pathref = 'E:/TextSummarization/donvanban/Summary_manual/' + idx \n",
    "rootbody = loadtxt(pathbody)\n",
    "refbody = loadtxt(pathref, ref=True)\n",
    "\n",
    "res = {'Rouge1_p':[], 'Rouge2_p':[], 'RougeL_p':[], 'RougeSU4_p': [],\n",
    "        'Rouge1_r':[], 'Rouge2_r':[], 'RougeL_r':[], 'RougeSU4_r': [],\n",
    "        'Rouge1_f':[], 'Rouge2_f':[], 'RougeL_f':[], 'RougeSU4_f': [],\n",
    "      }\n",
    "\n",
    "for idx in model_dict.keys():    \n",
    "    model = Summarizer(model = idx, sentence_handler=CoreferenceHandler())\n",
    "    all_result = []\n",
    "    for jdx in range(len(rootbody)):\n",
    "        result = model(rootbody[jdx], ratio=2*len(refbody[jdx])/len(rootbody[jdx]))\n",
    "        all_result.append(result)\n",
    "    r_rouge = rouge_dist(all_result, refbody)\n",
    "    for ind in ['p', 'r', 'f']:\n",
    "        res['Rouge1_'+ind].append(r_rouge['rouge-1'][ind])\n",
    "        res['Rouge2_'+ind].append(r_rouge['rouge-2'][ind])\n",
    "        res['RougeL_'+ind].append(r_rouge['rouge-l'][ind])\n",
    "        res['RougeSU4_'+ind].append(r_rouge['rouge-su4'][ind])\n",
    "df = pd.DataFrame(res, index = model_dict.keys())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T14:23:29.658089Z",
     "start_time": "2021-07-22T12:59:03.948560Z"
    },
    "code_folding": [
     8,
     13
    ],
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
      "c:\\users\\technical\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\transformers\\configuration_transfo_xl.py:146: FutureWarning: The config parameter `tie_weight` is deprecated. Please use `tie_word_embeddings` instead.\n",
      "  FutureWarning,\n",
      "c:\\users\\technical\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\transformers\\configuration_xlnet.py:212: FutureWarning: This config doesn't use attention memories, a core feature of XLNet. Consider setting `men_len` to a non-zero value, for example `xlnet = XLNetLMHeadModel.from_pretrained('xlnet-base-cased'', mem_len=1024)`, for accurate training performance as well as an order of magnitude faster inference. Starting from version 3.5.0, the default parameter will be 1024, following the implementation in https://arxiv.org/abs/1906.08237\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039e2f7f244248318ec0fcd827779578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=1452741, style=ProgressStyle(description_wi"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2abd089fae349749d0001c046ab3bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=1008321, style=ProgressStyle(description_wi"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4da2e07abf4fc9a2d85d310c7b51c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=442, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc1bc78be844421b7050669a22f0139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=267967963, style=ProgressStyle(description_"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4fef35fbcdb4b6883653b02ae054ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6f2a072cea4ff4afd97722c3d14cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=685, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb13f3c524f94afc82c55b7d34eec15d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=71509304, style=ProgressStyle(description_w"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0baae15dc046435d9a19be30d668c1f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=760289, style=ProgressStyle(description_wid"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89f727f1e834e13b921de499e38e425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=385, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bc3950151804987a0d438856c19c9df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=442221694, style=ProgressStyle(description_"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7077d854708240a9add8850ab872ac91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=227845, style=ProgressStyle(description_wid"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a038a43d5bc440fab6f8003af76df7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=558, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a986c345aa46e88bac3f3038487ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=1481467253, style=ProgressStyle(description"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3bb6e69926144c8ae7b6944821b662b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=895321, style=ProgressStyle(description_wid"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4240453793fc450b9bdab947d2238f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=1135173, style=ProgressStyle(description_wi"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rouge1_p</th>\n",
       "      <th>Rouge2_p</th>\n",
       "      <th>RougeL_p</th>\n",
       "      <th>RougeSU4_p</th>\n",
       "      <th>Rouge1_r</th>\n",
       "      <th>Rouge2_r</th>\n",
       "      <th>RougeL_r</th>\n",
       "      <th>RougeSU4_r</th>\n",
       "      <th>Rouge1_f</th>\n",
       "      <th>Rouge2_f</th>\n",
       "      <th>RougeL_f</th>\n",
       "      <th>RougeSU4_f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bert-base-multilingual-uncased</th>\n",
       "      <td>0.536254</td>\n",
       "      <td>0.368117</td>\n",
       "      <td>0.411873</td>\n",
       "      <td>0.349936</td>\n",
       "      <td>0.357484</td>\n",
       "      <td>0.245265</td>\n",
       "      <td>0.273270</td>\n",
       "      <td>0.230621</td>\n",
       "      <td>0.428990</td>\n",
       "      <td>0.294388</td>\n",
       "      <td>0.328552</td>\n",
       "      <td>0.278018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert-large-uncased</th>\n",
       "      <td>0.543157</td>\n",
       "      <td>0.354487</td>\n",
       "      <td>0.399662</td>\n",
       "      <td>0.332533</td>\n",
       "      <td>0.313515</td>\n",
       "      <td>0.207422</td>\n",
       "      <td>0.228703</td>\n",
       "      <td>0.193525</td>\n",
       "      <td>0.397557</td>\n",
       "      <td>0.261709</td>\n",
       "      <td>0.290926</td>\n",
       "      <td>0.244663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2</th>\n",
       "      <td>0.584382</td>\n",
       "      <td>0.411494</td>\n",
       "      <td>0.454490</td>\n",
       "      <td>0.389178</td>\n",
       "      <td>0.305186</td>\n",
       "      <td>0.210687</td>\n",
       "      <td>0.232031</td>\n",
       "      <td>0.195871</td>\n",
       "      <td>0.400970</td>\n",
       "      <td>0.278685</td>\n",
       "      <td>0.307218</td>\n",
       "      <td>0.260589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>facebook/bart-large</th>\n",
       "      <td>0.564344</td>\n",
       "      <td>0.378987</td>\n",
       "      <td>0.405870</td>\n",
       "      <td>0.356827</td>\n",
       "      <td>0.310637</td>\n",
       "      <td>0.207347</td>\n",
       "      <td>0.221083</td>\n",
       "      <td>0.192267</td>\n",
       "      <td>0.400708</td>\n",
       "      <td>0.268044</td>\n",
       "      <td>0.286245</td>\n",
       "      <td>0.249888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openai-gpt</th>\n",
       "      <td>0.570364</td>\n",
       "      <td>0.392951</td>\n",
       "      <td>0.418475</td>\n",
       "      <td>0.372536</td>\n",
       "      <td>0.318832</td>\n",
       "      <td>0.216094</td>\n",
       "      <td>0.232056</td>\n",
       "      <td>0.202027</td>\n",
       "      <td>0.409022</td>\n",
       "      <td>0.278845</td>\n",
       "      <td>0.298555</td>\n",
       "      <td>0.261981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ctrl</th>\n",
       "      <td>0.486931</td>\n",
       "      <td>0.319998</td>\n",
       "      <td>0.358052</td>\n",
       "      <td>0.305114</td>\n",
       "      <td>0.367720</td>\n",
       "      <td>0.250466</td>\n",
       "      <td>0.272123</td>\n",
       "      <td>0.236195</td>\n",
       "      <td>0.419012</td>\n",
       "      <td>0.280995</td>\n",
       "      <td>0.309229</td>\n",
       "      <td>0.266267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transfo-xl-wt103</th>\n",
       "      <td>0.583542</td>\n",
       "      <td>0.406037</td>\n",
       "      <td>0.456251</td>\n",
       "      <td>0.379149</td>\n",
       "      <td>0.262237</td>\n",
       "      <td>0.180220</td>\n",
       "      <td>0.199831</td>\n",
       "      <td>0.164662</td>\n",
       "      <td>0.361859</td>\n",
       "      <td>0.249638</td>\n",
       "      <td>0.277932</td>\n",
       "      <td>0.229607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xlnet-large-cased</th>\n",
       "      <td>0.525853</td>\n",
       "      <td>0.352307</td>\n",
       "      <td>0.391537</td>\n",
       "      <td>0.331034</td>\n",
       "      <td>0.352453</td>\n",
       "      <td>0.244075</td>\n",
       "      <td>0.266600</td>\n",
       "      <td>0.227216</td>\n",
       "      <td>0.422036</td>\n",
       "      <td>0.288370</td>\n",
       "      <td>0.317210</td>\n",
       "      <td>0.269471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xlm-mlm-enfr-1024</th>\n",
       "      <td>0.565920</td>\n",
       "      <td>0.388310</td>\n",
       "      <td>0.432547</td>\n",
       "      <td>0.367681</td>\n",
       "      <td>0.283499</td>\n",
       "      <td>0.191423</td>\n",
       "      <td>0.211416</td>\n",
       "      <td>0.177515</td>\n",
       "      <td>0.377759</td>\n",
       "      <td>0.256433</td>\n",
       "      <td>0.284015</td>\n",
       "      <td>0.239433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distilbert-base-uncased</th>\n",
       "      <td>0.539658</td>\n",
       "      <td>0.354672</td>\n",
       "      <td>0.397923</td>\n",
       "      <td>0.333094</td>\n",
       "      <td>0.315460</td>\n",
       "      <td>0.208399</td>\n",
       "      <td>0.230405</td>\n",
       "      <td>0.192865</td>\n",
       "      <td>0.398169</td>\n",
       "      <td>0.262536</td>\n",
       "      <td>0.291833</td>\n",
       "      <td>0.244286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>albert-large-v2</th>\n",
       "      <td>0.511611</td>\n",
       "      <td>0.331997</td>\n",
       "      <td>0.383816</td>\n",
       "      <td>0.316015</td>\n",
       "      <td>0.310654</td>\n",
       "      <td>0.204976</td>\n",
       "      <td>0.234063</td>\n",
       "      <td>0.191911</td>\n",
       "      <td>0.386577</td>\n",
       "      <td>0.253463</td>\n",
       "      <td>0.290792</td>\n",
       "      <td>0.238802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allenai/scibert_scivocab_uncased</th>\n",
       "      <td>0.514710</td>\n",
       "      <td>0.331589</td>\n",
       "      <td>0.366447</td>\n",
       "      <td>0.309732</td>\n",
       "      <td>0.311058</td>\n",
       "      <td>0.199946</td>\n",
       "      <td>0.218265</td>\n",
       "      <td>0.185468</td>\n",
       "      <td>0.387771</td>\n",
       "      <td>0.249466</td>\n",
       "      <td>0.273580</td>\n",
       "      <td>0.232009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vinai/phobert-large</th>\n",
       "      <td>0.475651</td>\n",
       "      <td>0.297596</td>\n",
       "      <td>0.341180</td>\n",
       "      <td>0.279125</td>\n",
       "      <td>0.392397</td>\n",
       "      <td>0.258927</td>\n",
       "      <td>0.286591</td>\n",
       "      <td>0.241661</td>\n",
       "      <td>0.430031</td>\n",
       "      <td>0.276918</td>\n",
       "      <td>0.311512</td>\n",
       "      <td>0.259045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Rouge1_p  Rouge2_p  RougeL_p  RougeSU4_p  \\\n",
       "bert-base-multilingual-uncased    0.536254  0.368117  0.411873    0.349936   \n",
       "bert-large-uncased                0.543157  0.354487  0.399662    0.332533   \n",
       "gpt2                              0.584382  0.411494  0.454490    0.389178   \n",
       "facebook/bart-large               0.564344  0.378987  0.405870    0.356827   \n",
       "openai-gpt                        0.570364  0.392951  0.418475    0.372536   \n",
       "ctrl                              0.486931  0.319998  0.358052    0.305114   \n",
       "transfo-xl-wt103                  0.583542  0.406037  0.456251    0.379149   \n",
       "xlnet-large-cased                 0.525853  0.352307  0.391537    0.331034   \n",
       "xlm-mlm-enfr-1024                 0.565920  0.388310  0.432547    0.367681   \n",
       "distilbert-base-uncased           0.539658  0.354672  0.397923    0.333094   \n",
       "albert-large-v2                   0.511611  0.331997  0.383816    0.316015   \n",
       "allenai/scibert_scivocab_uncased  0.514710  0.331589  0.366447    0.309732   \n",
       "vinai/phobert-large               0.475651  0.297596  0.341180    0.279125   \n",
       "\n",
       "                                  Rouge1_r  Rouge2_r  RougeL_r  RougeSU4_r  \\\n",
       "bert-base-multilingual-uncased    0.357484  0.245265  0.273270    0.230621   \n",
       "bert-large-uncased                0.313515  0.207422  0.228703    0.193525   \n",
       "gpt2                              0.305186  0.210687  0.232031    0.195871   \n",
       "facebook/bart-large               0.310637  0.207347  0.221083    0.192267   \n",
       "openai-gpt                        0.318832  0.216094  0.232056    0.202027   \n",
       "ctrl                              0.367720  0.250466  0.272123    0.236195   \n",
       "transfo-xl-wt103                  0.262237  0.180220  0.199831    0.164662   \n",
       "xlnet-large-cased                 0.352453  0.244075  0.266600    0.227216   \n",
       "xlm-mlm-enfr-1024                 0.283499  0.191423  0.211416    0.177515   \n",
       "distilbert-base-uncased           0.315460  0.208399  0.230405    0.192865   \n",
       "albert-large-v2                   0.310654  0.204976  0.234063    0.191911   \n",
       "allenai/scibert_scivocab_uncased  0.311058  0.199946  0.218265    0.185468   \n",
       "vinai/phobert-large               0.392397  0.258927  0.286591    0.241661   \n",
       "\n",
       "                                  Rouge1_f  Rouge2_f  RougeL_f  RougeSU4_f  \n",
       "bert-base-multilingual-uncased    0.428990  0.294388  0.328552    0.278018  \n",
       "bert-large-uncased                0.397557  0.261709  0.290926    0.244663  \n",
       "gpt2                              0.400970  0.278685  0.307218    0.260589  \n",
       "facebook/bart-large               0.400708  0.268044  0.286245    0.249888  \n",
       "openai-gpt                        0.409022  0.278845  0.298555    0.261981  \n",
       "ctrl                              0.419012  0.280995  0.309229    0.266267  \n",
       "transfo-xl-wt103                  0.361859  0.249638  0.277932    0.229607  \n",
       "xlnet-large-cased                 0.422036  0.288370  0.317210    0.269471  \n",
       "xlm-mlm-enfr-1024                 0.377759  0.256433  0.284015    0.239433  \n",
       "distilbert-base-uncased           0.398169  0.262536  0.291833    0.244286  \n",
       "albert-large-v2                   0.386577  0.253463  0.290792    0.238802  \n",
       "allenai/scibert_scivocab_uncased  0.387771  0.249466  0.273580    0.232009  \n",
       "vinai/phobert-large               0.430031  0.276918  0.311512    0.259045  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#khoahockythuat\n",
    "idx = category[2]\n",
    "\n",
    "pathbody = 'E:/TextSummarization/donvanban/Plaintext/' + idx \n",
    "pathref = 'E:/TextSummarization/donvanban/Summary_manual/' + idx \n",
    "rootbody = loadtxt(pathbody)\n",
    "refbody = loadtxt(pathref, ref=True)\n",
    "\n",
    "res = {'Rouge1_p':[], 'Rouge2_p':[], 'RougeL_p':[], 'RougeSU4_p': [],\n",
    "        'Rouge1_r':[], 'Rouge2_r':[], 'RougeL_r':[], 'RougeSU4_r': [],\n",
    "        'Rouge1_f':[], 'Rouge2_f':[], 'RougeL_f':[], 'RougeSU4_f': [],\n",
    "      }\n",
    "\n",
    "for idx in model_dict.keys():    \n",
    "    model = Summarizer(model = idx, sentence_handler=CoreferenceHandler())\n",
    "    all_result = []\n",
    "    for jdx in range(len(rootbody)):\n",
    "        result = model(rootbody[jdx], ratio=2*len(refbody[jdx])/len(rootbody[jdx]))\n",
    "        all_result.append(result)\n",
    "    r_rouge = rouge_dist(all_result, refbody)\n",
    "    for ind in ['p', 'r', 'f']:\n",
    "        res['Rouge1_'+ind].append(r_rouge['rouge-1'][ind])\n",
    "        res['Rouge2_'+ind].append(r_rouge['rouge-2'][ind])\n",
    "        res['RougeL_'+ind].append(r_rouge['rouge-l'][ind])\n",
    "        res['RougeSU4_'+ind].append(r_rouge['rouge-su4'][ind])\n",
    "df = pd.DataFrame(res, index = model_dict.keys())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T08:27:12.036362Z",
     "start_time": "2021-07-23T05:33:52.974766Z"
    },
    "code_folding": [
     7,
     12,
     19
    ],
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
      "c:\\users\\technical\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ..\\aten\\src\\ATen\\native\\BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      "c:\\users\\technical\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\transformers\\configuration_transfo_xl.py:146: FutureWarning: The config parameter `tie_weight` is deprecated. Please use `tie_word_embeddings` instead.\n",
      "  FutureWarning,\n",
      "c:\\users\\technical\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\transformers\\configuration_xlnet.py:212: FutureWarning: This config doesn't use attention memories, a core feature of XLNet. Consider setting `men_len` to a non-zero value, for example `xlnet = XLNetLMHeadModel.from_pretrained('xlnet-base-cased'', mem_len=1024)`, for accurate training performance as well as an order of magnitude faster inference. Starting from version 3.5.0, the default parameter will be 1024, following the implementation in https://arxiv.org/abs/1906.08237\n",
      "  FutureWarning,\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rouge1_p</th>\n",
       "      <th>Rouge2_p</th>\n",
       "      <th>RougeL_p</th>\n",
       "      <th>RougeSU4_p</th>\n",
       "      <th>Rouge1_r</th>\n",
       "      <th>Rouge2_r</th>\n",
       "      <th>RougeL_r</th>\n",
       "      <th>RougeSU4_r</th>\n",
       "      <th>Rouge1_f</th>\n",
       "      <th>Rouge2_f</th>\n",
       "      <th>RougeL_f</th>\n",
       "      <th>RougeSU4_f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bert-base-multilingual-uncased</th>\n",
       "      <td>0.530388</td>\n",
       "      <td>0.356513</td>\n",
       "      <td>0.405619</td>\n",
       "      <td>0.333932</td>\n",
       "      <td>0.299726</td>\n",
       "      <td>0.200465</td>\n",
       "      <td>0.226252</td>\n",
       "      <td>0.185960</td>\n",
       "      <td>0.383010</td>\n",
       "      <td>0.256629</td>\n",
       "      <td>0.290477</td>\n",
       "      <td>0.238888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert-large-uncased</th>\n",
       "      <td>0.492336</td>\n",
       "      <td>0.335471</td>\n",
       "      <td>0.383629</td>\n",
       "      <td>0.317570</td>\n",
       "      <td>0.276358</td>\n",
       "      <td>0.186792</td>\n",
       "      <td>0.212648</td>\n",
       "      <td>0.174713</td>\n",
       "      <td>0.354006</td>\n",
       "      <td>0.239968</td>\n",
       "      <td>0.273624</td>\n",
       "      <td>0.225414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2</th>\n",
       "      <td>0.569050</td>\n",
       "      <td>0.409568</td>\n",
       "      <td>0.450531</td>\n",
       "      <td>0.388871</td>\n",
       "      <td>0.280465</td>\n",
       "      <td>0.198937</td>\n",
       "      <td>0.219082</td>\n",
       "      <td>0.185618</td>\n",
       "      <td>0.375741</td>\n",
       "      <td>0.267798</td>\n",
       "      <td>0.294806</td>\n",
       "      <td>0.251289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>facebook/bart-large</th>\n",
       "      <td>0.518434</td>\n",
       "      <td>0.340215</td>\n",
       "      <td>0.400308</td>\n",
       "      <td>0.324933</td>\n",
       "      <td>0.286019</td>\n",
       "      <td>0.190926</td>\n",
       "      <td>0.217445</td>\n",
       "      <td>0.179552</td>\n",
       "      <td>0.368653</td>\n",
       "      <td>0.244590</td>\n",
       "      <td>0.281812</td>\n",
       "      <td>0.231295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openai-gpt</th>\n",
       "      <td>0.570235</td>\n",
       "      <td>0.406665</td>\n",
       "      <td>0.456003</td>\n",
       "      <td>0.385390</td>\n",
       "      <td>0.300957</td>\n",
       "      <td>0.214085</td>\n",
       "      <td>0.238028</td>\n",
       "      <td>0.198954</td>\n",
       "      <td>0.393980</td>\n",
       "      <td>0.280502</td>\n",
       "      <td>0.312786</td>\n",
       "      <td>0.262431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ctrl</th>\n",
       "      <td>0.560080</td>\n",
       "      <td>0.408440</td>\n",
       "      <td>0.443820</td>\n",
       "      <td>0.393660</td>\n",
       "      <td>0.302701</td>\n",
       "      <td>0.212283</td>\n",
       "      <td>0.233710</td>\n",
       "      <td>0.201295</td>\n",
       "      <td>0.393001</td>\n",
       "      <td>0.279368</td>\n",
       "      <td>0.306186</td>\n",
       "      <td>0.266379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transfo-xl-wt103</th>\n",
       "      <td>0.529068</td>\n",
       "      <td>0.351818</td>\n",
       "      <td>0.417442</td>\n",
       "      <td>0.338171</td>\n",
       "      <td>0.252087</td>\n",
       "      <td>0.173870</td>\n",
       "      <td>0.200028</td>\n",
       "      <td>0.164086</td>\n",
       "      <td>0.341472</td>\n",
       "      <td>0.232726</td>\n",
       "      <td>0.270459</td>\n",
       "      <td>0.220959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xlnet-large-cased</th>\n",
       "      <td>0.557770</td>\n",
       "      <td>0.394704</td>\n",
       "      <td>0.439757</td>\n",
       "      <td>0.380134</td>\n",
       "      <td>0.324983</td>\n",
       "      <td>0.228656</td>\n",
       "      <td>0.253074</td>\n",
       "      <td>0.217262</td>\n",
       "      <td>0.410683</td>\n",
       "      <td>0.289564</td>\n",
       "      <td>0.321264</td>\n",
       "      <td>0.276495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xlm-mlm-enfr-1024</th>\n",
       "      <td>0.533898</td>\n",
       "      <td>0.359918</td>\n",
       "      <td>0.417652</td>\n",
       "      <td>0.339807</td>\n",
       "      <td>0.272771</td>\n",
       "      <td>0.183817</td>\n",
       "      <td>0.211893</td>\n",
       "      <td>0.171892</td>\n",
       "      <td>0.361069</td>\n",
       "      <td>0.243350</td>\n",
       "      <td>0.281148</td>\n",
       "      <td>0.228299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distilbert-base-uncased</th>\n",
       "      <td>0.553853</td>\n",
       "      <td>0.388896</td>\n",
       "      <td>0.441077</td>\n",
       "      <td>0.372155</td>\n",
       "      <td>0.304812</td>\n",
       "      <td>0.213395</td>\n",
       "      <td>0.241322</td>\n",
       "      <td>0.201049</td>\n",
       "      <td>0.393217</td>\n",
       "      <td>0.275576</td>\n",
       "      <td>0.311963</td>\n",
       "      <td>0.261064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>albert-large-v2</th>\n",
       "      <td>0.494522</td>\n",
       "      <td>0.321914</td>\n",
       "      <td>0.381601</td>\n",
       "      <td>0.310072</td>\n",
       "      <td>0.260761</td>\n",
       "      <td>0.171580</td>\n",
       "      <td>0.200217</td>\n",
       "      <td>0.161604</td>\n",
       "      <td>0.341467</td>\n",
       "      <td>0.223849</td>\n",
       "      <td>0.262635</td>\n",
       "      <td>0.212471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allenai/scibert_scivocab_uncased</th>\n",
       "      <td>0.546434</td>\n",
       "      <td>0.393624</td>\n",
       "      <td>0.432124</td>\n",
       "      <td>0.371290</td>\n",
       "      <td>0.304888</td>\n",
       "      <td>0.214258</td>\n",
       "      <td>0.234931</td>\n",
       "      <td>0.199418</td>\n",
       "      <td>0.391394</td>\n",
       "      <td>0.277479</td>\n",
       "      <td>0.304381</td>\n",
       "      <td>0.259474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vinai/phobert-large</th>\n",
       "      <td>0.574526</td>\n",
       "      <td>0.426339</td>\n",
       "      <td>0.459586</td>\n",
       "      <td>0.408102</td>\n",
       "      <td>0.327276</td>\n",
       "      <td>0.232051</td>\n",
       "      <td>0.253609</td>\n",
       "      <td>0.218065</td>\n",
       "      <td>0.417006</td>\n",
       "      <td>0.300528</td>\n",
       "      <td>0.326854</td>\n",
       "      <td>0.284246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Rouge1_p  Rouge2_p  RougeL_p  RougeSU4_p  \\\n",
       "bert-base-multilingual-uncased    0.530388  0.356513  0.405619    0.333932   \n",
       "bert-large-uncased                0.492336  0.335471  0.383629    0.317570   \n",
       "gpt2                              0.569050  0.409568  0.450531    0.388871   \n",
       "facebook/bart-large               0.518434  0.340215  0.400308    0.324933   \n",
       "openai-gpt                        0.570235  0.406665  0.456003    0.385390   \n",
       "ctrl                              0.560080  0.408440  0.443820    0.393660   \n",
       "transfo-xl-wt103                  0.529068  0.351818  0.417442    0.338171   \n",
       "xlnet-large-cased                 0.557770  0.394704  0.439757    0.380134   \n",
       "xlm-mlm-enfr-1024                 0.533898  0.359918  0.417652    0.339807   \n",
       "distilbert-base-uncased           0.553853  0.388896  0.441077    0.372155   \n",
       "albert-large-v2                   0.494522  0.321914  0.381601    0.310072   \n",
       "allenai/scibert_scivocab_uncased  0.546434  0.393624  0.432124    0.371290   \n",
       "vinai/phobert-large               0.574526  0.426339  0.459586    0.408102   \n",
       "\n",
       "                                  Rouge1_r  Rouge2_r  RougeL_r  RougeSU4_r  \\\n",
       "bert-base-multilingual-uncased    0.299726  0.200465  0.226252    0.185960   \n",
       "bert-large-uncased                0.276358  0.186792  0.212648    0.174713   \n",
       "gpt2                              0.280465  0.198937  0.219082    0.185618   \n",
       "facebook/bart-large               0.286019  0.190926  0.217445    0.179552   \n",
       "openai-gpt                        0.300957  0.214085  0.238028    0.198954   \n",
       "ctrl                              0.302701  0.212283  0.233710    0.201295   \n",
       "transfo-xl-wt103                  0.252087  0.173870  0.200028    0.164086   \n",
       "xlnet-large-cased                 0.324983  0.228656  0.253074    0.217262   \n",
       "xlm-mlm-enfr-1024                 0.272771  0.183817  0.211893    0.171892   \n",
       "distilbert-base-uncased           0.304812  0.213395  0.241322    0.201049   \n",
       "albert-large-v2                   0.260761  0.171580  0.200217    0.161604   \n",
       "allenai/scibert_scivocab_uncased  0.304888  0.214258  0.234931    0.199418   \n",
       "vinai/phobert-large               0.327276  0.232051  0.253609    0.218065   \n",
       "\n",
       "                                  Rouge1_f  Rouge2_f  RougeL_f  RougeSU4_f  \n",
       "bert-base-multilingual-uncased    0.383010  0.256629  0.290477    0.238888  \n",
       "bert-large-uncased                0.354006  0.239968  0.273624    0.225414  \n",
       "gpt2                              0.375741  0.267798  0.294806    0.251289  \n",
       "facebook/bart-large               0.368653  0.244590  0.281812    0.231295  \n",
       "openai-gpt                        0.393980  0.280502  0.312786    0.262431  \n",
       "ctrl                              0.393001  0.279368  0.306186    0.266379  \n",
       "transfo-xl-wt103                  0.341472  0.232726  0.270459    0.220959  \n",
       "xlnet-large-cased                 0.410683  0.289564  0.321264    0.276495  \n",
       "xlm-mlm-enfr-1024                 0.361069  0.243350  0.281148    0.228299  \n",
       "distilbert-base-uncased           0.393217  0.275576  0.311963    0.261064  \n",
       "albert-large-v2                   0.341467  0.223849  0.262635    0.212471  \n",
       "allenai/scibert_scivocab_uncased  0.391394  0.277479  0.304381    0.259474  \n",
       "vinai/phobert-large               0.417006  0.300528  0.326854    0.284246  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = category[3]\n",
    "\n",
    "pathbody = 'E:/TextSummarization/donvanban/Plaintext/' + idx \n",
    "pathref = 'E:/TextSummarization/donvanban/Summary_manual/' + idx \n",
    "rootbody = loadtxt(pathbody)\n",
    "refbody = loadtxt(pathref, ref=True)\n",
    "\n",
    "res = {'Rouge1_p':[], 'Rouge2_p':[], 'RougeL_p':[], 'RougeSU4_p': [],\n",
    "        'Rouge1_r':[], 'Rouge2_r':[], 'RougeL_r':[], 'RougeSU4_r': [],\n",
    "        'Rouge1_f':[], 'Rouge2_f':[], 'RougeL_f':[], 'RougeSU4_f': [],\n",
    "      }\n",
    "\n",
    "for idx in model_dict.keys():    \n",
    "    model = Summarizer(model = idx, sentence_handler=CoreferenceHandler())\n",
    "    all_result = []\n",
    "    for jdx in range(len(rootbody)):\n",
    "        result = model(rootbody[jdx], ratio=2*len(refbody[jdx])/len(rootbody[jdx]))\n",
    "        all_result.append(result)\n",
    "    r_rouge = rouge_dist(all_result, refbody)\n",
    "    for ind in ['p', 'r', 'f']:\n",
    "        res['Rouge1_'+ind].append(r_rouge['rouge-1'][ind])\n",
    "        res['Rouge2_'+ind].append(r_rouge['rouge-2'][ind])\n",
    "        res['RougeL_'+ind].append(r_rouge['rouge-l'][ind])\n",
    "        res['RougeSU4_'+ind].append(r_rouge['rouge-su4'][ind])\n",
    "df = pd.DataFrame(res, index = model_dict.keys())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T07:28:47.570139Z",
     "start_time": "2021-07-23T05:51:49.333787Z"
    },
    "code_folding": [
     7,
     12
    ],
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
      "c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ..\\aten\\src\\ATen\\native\\BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      "c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\transformers\\configuration_transfo_xl.py:146: FutureWarning: The config parameter `tie_weight` is deprecated. Please use `tie_word_embeddings` instead.\n",
      "  FutureWarning,\n",
      "c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\transformers\\configuration_xlnet.py:212: FutureWarning: This config doesn't use attention memories, a core feature of XLNet. Consider setting `men_len` to a non-zero value, for example `xlnet = XLNetLMHeadModel.from_pretrained('xlnet-base-cased'', mem_len=1024)`, for accurate training performance as well as an order of magnitude faster inference. Starting from version 3.5.0, the default parameter will be 1024, following the implementation in https://arxiv.org/abs/1906.08237\n",
      "  FutureWarning,\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rouge1_p</th>\n",
       "      <th>Rouge2_p</th>\n",
       "      <th>RougeL_p</th>\n",
       "      <th>RougeSU4_p</th>\n",
       "      <th>Rouge1_r</th>\n",
       "      <th>Rouge2_r</th>\n",
       "      <th>RougeL_r</th>\n",
       "      <th>RougeSU4_r</th>\n",
       "      <th>Rouge1_f</th>\n",
       "      <th>Rouge2_f</th>\n",
       "      <th>RougeL_f</th>\n",
       "      <th>RougeSU4_f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bert-base-multilingual-uncased</th>\n",
       "      <td>0.419959</td>\n",
       "      <td>0.260637</td>\n",
       "      <td>0.318426</td>\n",
       "      <td>0.246730</td>\n",
       "      <td>0.251632</td>\n",
       "      <td>0.160160</td>\n",
       "      <td>0.189027</td>\n",
       "      <td>0.151359</td>\n",
       "      <td>0.314701</td>\n",
       "      <td>0.198403</td>\n",
       "      <td>0.237228</td>\n",
       "      <td>0.187620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert-large-uncased</th>\n",
       "      <td>0.408568</td>\n",
       "      <td>0.254353</td>\n",
       "      <td>0.307670</td>\n",
       "      <td>0.246895</td>\n",
       "      <td>0.208488</td>\n",
       "      <td>0.126279</td>\n",
       "      <td>0.152615</td>\n",
       "      <td>0.120526</td>\n",
       "      <td>0.276090</td>\n",
       "      <td>0.168769</td>\n",
       "      <td>0.204026</td>\n",
       "      <td>0.161979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2</th>\n",
       "      <td>0.433021</td>\n",
       "      <td>0.277941</td>\n",
       "      <td>0.332966</td>\n",
       "      <td>0.265527</td>\n",
       "      <td>0.215373</td>\n",
       "      <td>0.139819</td>\n",
       "      <td>0.165171</td>\n",
       "      <td>0.132079</td>\n",
       "      <td>0.287667</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.220808</td>\n",
       "      <td>0.176408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>facebook/bart-large</th>\n",
       "      <td>0.421589</td>\n",
       "      <td>0.270862</td>\n",
       "      <td>0.328721</td>\n",
       "      <td>0.260702</td>\n",
       "      <td>0.225046</td>\n",
       "      <td>0.150777</td>\n",
       "      <td>0.175961</td>\n",
       "      <td>0.143450</td>\n",
       "      <td>0.293448</td>\n",
       "      <td>0.193719</td>\n",
       "      <td>0.229222</td>\n",
       "      <td>0.185067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openai-gpt</th>\n",
       "      <td>0.443905</td>\n",
       "      <td>0.292327</td>\n",
       "      <td>0.340738</td>\n",
       "      <td>0.282514</td>\n",
       "      <td>0.237590</td>\n",
       "      <td>0.161473</td>\n",
       "      <td>0.181809</td>\n",
       "      <td>0.153630</td>\n",
       "      <td>0.309518</td>\n",
       "      <td>0.208034</td>\n",
       "      <td>0.237105</td>\n",
       "      <td>0.199029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ctrl</th>\n",
       "      <td>0.407358</td>\n",
       "      <td>0.253398</td>\n",
       "      <td>0.304184</td>\n",
       "      <td>0.241371</td>\n",
       "      <td>0.240232</td>\n",
       "      <td>0.144653</td>\n",
       "      <td>0.171588</td>\n",
       "      <td>0.136343</td>\n",
       "      <td>0.302229</td>\n",
       "      <td>0.184171</td>\n",
       "      <td>0.219409</td>\n",
       "      <td>0.174255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transfo-xl-wt103</th>\n",
       "      <td>0.438544</td>\n",
       "      <td>0.266815</td>\n",
       "      <td>0.335239</td>\n",
       "      <td>0.255635</td>\n",
       "      <td>0.218167</td>\n",
       "      <td>0.138108</td>\n",
       "      <td>0.165696</td>\n",
       "      <td>0.131283</td>\n",
       "      <td>0.291379</td>\n",
       "      <td>0.182007</td>\n",
       "      <td>0.221776</td>\n",
       "      <td>0.173476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xlnet-large-cased</th>\n",
       "      <td>0.381087</td>\n",
       "      <td>0.219418</td>\n",
       "      <td>0.273056</td>\n",
       "      <td>0.210942</td>\n",
       "      <td>0.245915</td>\n",
       "      <td>0.141625</td>\n",
       "      <td>0.172453</td>\n",
       "      <td>0.135256</td>\n",
       "      <td>0.298931</td>\n",
       "      <td>0.172141</td>\n",
       "      <td>0.211396</td>\n",
       "      <td>0.164826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xlm-mlm-enfr-1024</th>\n",
       "      <td>0.410395</td>\n",
       "      <td>0.244495</td>\n",
       "      <td>0.307248</td>\n",
       "      <td>0.234934</td>\n",
       "      <td>0.220099</td>\n",
       "      <td>0.130600</td>\n",
       "      <td>0.161009</td>\n",
       "      <td>0.124493</td>\n",
       "      <td>0.286530</td>\n",
       "      <td>0.170255</td>\n",
       "      <td>0.211293</td>\n",
       "      <td>0.162746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distilbert-base-uncased</th>\n",
       "      <td>0.421393</td>\n",
       "      <td>0.253721</td>\n",
       "      <td>0.314517</td>\n",
       "      <td>0.246651</td>\n",
       "      <td>0.229169</td>\n",
       "      <td>0.145959</td>\n",
       "      <td>0.174464</td>\n",
       "      <td>0.139197</td>\n",
       "      <td>0.296883</td>\n",
       "      <td>0.185313</td>\n",
       "      <td>0.224433</td>\n",
       "      <td>0.177962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>albert-large-v2</th>\n",
       "      <td>0.418416</td>\n",
       "      <td>0.259309</td>\n",
       "      <td>0.312746</td>\n",
       "      <td>0.243374</td>\n",
       "      <td>0.239711</td>\n",
       "      <td>0.154806</td>\n",
       "      <td>0.178794</td>\n",
       "      <td>0.144428</td>\n",
       "      <td>0.304801</td>\n",
       "      <td>0.193871</td>\n",
       "      <td>0.227518</td>\n",
       "      <td>0.181278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allenai/scibert_scivocab_uncased</th>\n",
       "      <td>0.409715</td>\n",
       "      <td>0.251090</td>\n",
       "      <td>0.309264</td>\n",
       "      <td>0.239114</td>\n",
       "      <td>0.237339</td>\n",
       "      <td>0.151423</td>\n",
       "      <td>0.179944</td>\n",
       "      <td>0.144180</td>\n",
       "      <td>0.300567</td>\n",
       "      <td>0.188917</td>\n",
       "      <td>0.227511</td>\n",
       "      <td>0.179891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vinai/phobert-large</th>\n",
       "      <td>0.424669</td>\n",
       "      <td>0.265593</td>\n",
       "      <td>0.323493</td>\n",
       "      <td>0.249347</td>\n",
       "      <td>0.260240</td>\n",
       "      <td>0.170680</td>\n",
       "      <td>0.198443</td>\n",
       "      <td>0.160637</td>\n",
       "      <td>0.322717</td>\n",
       "      <td>0.207812</td>\n",
       "      <td>0.245988</td>\n",
       "      <td>0.195395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Rouge1_p  Rouge2_p  RougeL_p  RougeSU4_p  \\\n",
       "bert-base-multilingual-uncased    0.419959  0.260637  0.318426    0.246730   \n",
       "bert-large-uncased                0.408568  0.254353  0.307670    0.246895   \n",
       "gpt2                              0.433021  0.277941  0.332966    0.265527   \n",
       "facebook/bart-large               0.421589  0.270862  0.328721    0.260702   \n",
       "openai-gpt                        0.443905  0.292327  0.340738    0.282514   \n",
       "ctrl                              0.407358  0.253398  0.304184    0.241371   \n",
       "transfo-xl-wt103                  0.438544  0.266815  0.335239    0.255635   \n",
       "xlnet-large-cased                 0.381087  0.219418  0.273056    0.210942   \n",
       "xlm-mlm-enfr-1024                 0.410395  0.244495  0.307248    0.234934   \n",
       "distilbert-base-uncased           0.421393  0.253721  0.314517    0.246651   \n",
       "albert-large-v2                   0.418416  0.259309  0.312746    0.243374   \n",
       "allenai/scibert_scivocab_uncased  0.409715  0.251090  0.309264    0.239114   \n",
       "vinai/phobert-large               0.424669  0.265593  0.323493    0.249347   \n",
       "\n",
       "                                  Rouge1_r  Rouge2_r  RougeL_r  RougeSU4_r  \\\n",
       "bert-base-multilingual-uncased    0.251632  0.160160  0.189027    0.151359   \n",
       "bert-large-uncased                0.208488  0.126279  0.152615    0.120526   \n",
       "gpt2                              0.215373  0.139819  0.165171    0.132079   \n",
       "facebook/bart-large               0.225046  0.150777  0.175961    0.143450   \n",
       "openai-gpt                        0.237590  0.161473  0.181809    0.153630   \n",
       "ctrl                              0.240232  0.144653  0.171588    0.136343   \n",
       "transfo-xl-wt103                  0.218167  0.138108  0.165696    0.131283   \n",
       "xlnet-large-cased                 0.245915  0.141625  0.172453    0.135256   \n",
       "xlm-mlm-enfr-1024                 0.220099  0.130600  0.161009    0.124493   \n",
       "distilbert-base-uncased           0.229169  0.145959  0.174464    0.139197   \n",
       "albert-large-v2                   0.239711  0.154806  0.178794    0.144428   \n",
       "allenai/scibert_scivocab_uncased  0.237339  0.151423  0.179944    0.144180   \n",
       "vinai/phobert-large               0.260240  0.170680  0.198443    0.160637   \n",
       "\n",
       "                                  Rouge1_f  Rouge2_f  RougeL_f  RougeSU4_f  \n",
       "bert-base-multilingual-uncased    0.314701  0.198403  0.237228    0.187620  \n",
       "bert-large-uncased                0.276090  0.168769  0.204026    0.161979  \n",
       "gpt2                              0.287667  0.186047  0.220808    0.176408  \n",
       "facebook/bart-large               0.293448  0.193719  0.229222    0.185067  \n",
       "openai-gpt                        0.309518  0.208034  0.237105    0.199029  \n",
       "ctrl                              0.302229  0.184171  0.219409    0.174255  \n",
       "transfo-xl-wt103                  0.291379  0.182007  0.221776    0.173476  \n",
       "xlnet-large-cased                 0.298931  0.172141  0.211396    0.164826  \n",
       "xlm-mlm-enfr-1024                 0.286530  0.170255  0.211293    0.162746  \n",
       "distilbert-base-uncased           0.296883  0.185313  0.224433    0.177962  \n",
       "albert-large-v2                   0.304801  0.193871  0.227518    0.181278  \n",
       "allenai/scibert_scivocab_uncased  0.300567  0.188917  0.227511    0.179891  \n",
       "vinai/phobert-large               0.322717  0.207812  0.245988    0.195395  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = category[4]\n",
    "\n",
    "pathbody = 'E:/TextSummarization/donvanban/Plaintext/' + idx \n",
    "pathref = 'E:/TextSummarization/donvanban/Summary_manual/' + idx \n",
    "rootbody = loadtxt(pathbody)\n",
    "refbody = loadtxt(pathref, ref=True)\n",
    "\n",
    "res = {'Rouge1_p':[], 'Rouge2_p':[], 'RougeL_p':[], 'RougeSU4_p': [],\n",
    "        'Rouge1_r':[], 'Rouge2_r':[], 'RougeL_r':[], 'RougeSU4_r': [],\n",
    "        'Rouge1_f':[], 'Rouge2_f':[], 'RougeL_f':[], 'RougeSU4_f': [],\n",
    "      }\n",
    "\n",
    "for idx in model_dict.keys():    \n",
    "    model = Summarizer(model = idx, sentence_handler=CoreferenceHandler())\n",
    "    all_result = []\n",
    "    for jdx in range(len(rootbody)):\n",
    "        result = model(rootbody[jdx], ratio=2*len(refbody[jdx])/len(rootbody[jdx]))\n",
    "        all_result.append(result)\n",
    "    r_rouge = rouge_dist(all_result, refbody)\n",
    "    for ind in ['p', 'r', 'f']:\n",
    "        res['Rouge1_'+ind].append(r_rouge['rouge-1'][ind])\n",
    "        res['Rouge2_'+ind].append(r_rouge['rouge-2'][ind])\n",
    "        res['RougeL_'+ind].append(r_rouge['rouge-l'][ind])\n",
    "        res['RougeSU4_'+ind].append(r_rouge['rouge-su4'][ind])\n",
    "df = pd.DataFrame(res, index = model_dict.keys())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:23:53.443017Z",
     "start_time": "2021-07-23T08:27:12.041905Z"
    },
    "code_folding": [
     7,
     12
    ],
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
      "c:\\users\\technical\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\transformers\\configuration_transfo_xl.py:146: FutureWarning: The config parameter `tie_weight` is deprecated. Please use `tie_word_embeddings` instead.\n",
      "  FutureWarning,\n",
      "c:\\users\\technical\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\transformers\\configuration_xlnet.py:212: FutureWarning: This config doesn't use attention memories, a core feature of XLNet. Consider setting `men_len` to a non-zero value, for example `xlnet = XLNetLMHeadModel.from_pretrained('xlnet-base-cased'', mem_len=1024)`, for accurate training performance as well as an order of magnitude faster inference. Starting from version 3.5.0, the default parameter will be 1024, following the implementation in https://arxiv.org/abs/1906.08237\n",
      "  FutureWarning,\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rouge1_p</th>\n",
       "      <th>Rouge2_p</th>\n",
       "      <th>RougeL_p</th>\n",
       "      <th>RougeSU4_p</th>\n",
       "      <th>Rouge1_r</th>\n",
       "      <th>Rouge2_r</th>\n",
       "      <th>RougeL_r</th>\n",
       "      <th>RougeSU4_r</th>\n",
       "      <th>Rouge1_f</th>\n",
       "      <th>Rouge2_f</th>\n",
       "      <th>RougeL_f</th>\n",
       "      <th>RougeSU4_f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bert-base-multilingual-uncased</th>\n",
       "      <td>0.500717</td>\n",
       "      <td>0.335891</td>\n",
       "      <td>0.377074</td>\n",
       "      <td>0.319064</td>\n",
       "      <td>0.304166</td>\n",
       "      <td>0.211028</td>\n",
       "      <td>0.231594</td>\n",
       "      <td>0.198776</td>\n",
       "      <td>0.378443</td>\n",
       "      <td>0.259207</td>\n",
       "      <td>0.286948</td>\n",
       "      <td>0.244950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert-large-uncased</th>\n",
       "      <td>0.503477</td>\n",
       "      <td>0.344400</td>\n",
       "      <td>0.380821</td>\n",
       "      <td>0.328923</td>\n",
       "      <td>0.258243</td>\n",
       "      <td>0.176876</td>\n",
       "      <td>0.195496</td>\n",
       "      <td>0.165626</td>\n",
       "      <td>0.341384</td>\n",
       "      <td>0.233719</td>\n",
       "      <td>0.258361</td>\n",
       "      <td>0.220315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2</th>\n",
       "      <td>0.540692</td>\n",
       "      <td>0.380299</td>\n",
       "      <td>0.423641</td>\n",
       "      <td>0.365884</td>\n",
       "      <td>0.267464</td>\n",
       "      <td>0.189232</td>\n",
       "      <td>0.209020</td>\n",
       "      <td>0.178608</td>\n",
       "      <td>0.357890</td>\n",
       "      <td>0.252715</td>\n",
       "      <td>0.279927</td>\n",
       "      <td>0.240039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>facebook/bart-large</th>\n",
       "      <td>0.531781</td>\n",
       "      <td>0.364206</td>\n",
       "      <td>0.416920</td>\n",
       "      <td>0.350735</td>\n",
       "      <td>0.294335</td>\n",
       "      <td>0.207849</td>\n",
       "      <td>0.230653</td>\n",
       "      <td>0.197494</td>\n",
       "      <td>0.378934</td>\n",
       "      <td>0.264659</td>\n",
       "      <td>0.296998</td>\n",
       "      <td>0.252698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openai-gpt</th>\n",
       "      <td>0.487840</td>\n",
       "      <td>0.317110</td>\n",
       "      <td>0.370046</td>\n",
       "      <td>0.306068</td>\n",
       "      <td>0.267715</td>\n",
       "      <td>0.175316</td>\n",
       "      <td>0.201075</td>\n",
       "      <td>0.165604</td>\n",
       "      <td>0.345712</td>\n",
       "      <td>0.225798</td>\n",
       "      <td>0.260565</td>\n",
       "      <td>0.214921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ctrl</th>\n",
       "      <td>0.507409</td>\n",
       "      <td>0.356920</td>\n",
       "      <td>0.376180</td>\n",
       "      <td>0.341621</td>\n",
       "      <td>0.305062</td>\n",
       "      <td>0.211551</td>\n",
       "      <td>0.223330</td>\n",
       "      <td>0.199203</td>\n",
       "      <td>0.381038</td>\n",
       "      <td>0.265649</td>\n",
       "      <td>0.280269</td>\n",
       "      <td>0.251660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transfo-xl-wt103</th>\n",
       "      <td>0.525901</td>\n",
       "      <td>0.352383</td>\n",
       "      <td>0.398021</td>\n",
       "      <td>0.334715</td>\n",
       "      <td>0.245627</td>\n",
       "      <td>0.168283</td>\n",
       "      <td>0.186899</td>\n",
       "      <td>0.156607</td>\n",
       "      <td>0.334856</td>\n",
       "      <td>0.227786</td>\n",
       "      <td>0.254359</td>\n",
       "      <td>0.213378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xlnet-large-cased</th>\n",
       "      <td>0.519272</td>\n",
       "      <td>0.374568</td>\n",
       "      <td>0.402207</td>\n",
       "      <td>0.353530</td>\n",
       "      <td>0.294641</td>\n",
       "      <td>0.215823</td>\n",
       "      <td>0.229381</td>\n",
       "      <td>0.200726</td>\n",
       "      <td>0.375959</td>\n",
       "      <td>0.273854</td>\n",
       "      <td>0.292148</td>\n",
       "      <td>0.256065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xlm-mlm-enfr-1024</th>\n",
       "      <td>0.524133</td>\n",
       "      <td>0.371204</td>\n",
       "      <td>0.409127</td>\n",
       "      <td>0.353783</td>\n",
       "      <td>0.306791</td>\n",
       "      <td>0.223627</td>\n",
       "      <td>0.241264</td>\n",
       "      <td>0.209859</td>\n",
       "      <td>0.387038</td>\n",
       "      <td>0.279109</td>\n",
       "      <td>0.303533</td>\n",
       "      <td>0.263446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distilbert-base-uncased</th>\n",
       "      <td>0.504549</td>\n",
       "      <td>0.329222</td>\n",
       "      <td>0.376833</td>\n",
       "      <td>0.316757</td>\n",
       "      <td>0.288143</td>\n",
       "      <td>0.195924</td>\n",
       "      <td>0.215930</td>\n",
       "      <td>0.185842</td>\n",
       "      <td>0.366806</td>\n",
       "      <td>0.245656</td>\n",
       "      <td>0.274543</td>\n",
       "      <td>0.234249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>albert-large-v2</th>\n",
       "      <td>0.480543</td>\n",
       "      <td>0.307515</td>\n",
       "      <td>0.354598</td>\n",
       "      <td>0.292581</td>\n",
       "      <td>0.256889</td>\n",
       "      <td>0.165319</td>\n",
       "      <td>0.190731</td>\n",
       "      <td>0.155124</td>\n",
       "      <td>0.334800</td>\n",
       "      <td>0.215036</td>\n",
       "      <td>0.248044</td>\n",
       "      <td>0.202751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allenai/scibert_scivocab_uncased</th>\n",
       "      <td>0.485229</td>\n",
       "      <td>0.323698</td>\n",
       "      <td>0.363979</td>\n",
       "      <td>0.305105</td>\n",
       "      <td>0.268949</td>\n",
       "      <td>0.178652</td>\n",
       "      <td>0.200715</td>\n",
       "      <td>0.165867</td>\n",
       "      <td>0.346077</td>\n",
       "      <td>0.230235</td>\n",
       "      <td>0.258746</td>\n",
       "      <td>0.214904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vinai/phobert-large</th>\n",
       "      <td>0.486945</td>\n",
       "      <td>0.320247</td>\n",
       "      <td>0.364166</td>\n",
       "      <td>0.303000</td>\n",
       "      <td>0.307452</td>\n",
       "      <td>0.208641</td>\n",
       "      <td>0.230677</td>\n",
       "      <td>0.194424</td>\n",
       "      <td>0.376920</td>\n",
       "      <td>0.252668</td>\n",
       "      <td>0.282443</td>\n",
       "      <td>0.236862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Rouge1_p  Rouge2_p  RougeL_p  RougeSU4_p  \\\n",
       "bert-base-multilingual-uncased    0.500717  0.335891  0.377074    0.319064   \n",
       "bert-large-uncased                0.503477  0.344400  0.380821    0.328923   \n",
       "gpt2                              0.540692  0.380299  0.423641    0.365884   \n",
       "facebook/bart-large               0.531781  0.364206  0.416920    0.350735   \n",
       "openai-gpt                        0.487840  0.317110  0.370046    0.306068   \n",
       "ctrl                              0.507409  0.356920  0.376180    0.341621   \n",
       "transfo-xl-wt103                  0.525901  0.352383  0.398021    0.334715   \n",
       "xlnet-large-cased                 0.519272  0.374568  0.402207    0.353530   \n",
       "xlm-mlm-enfr-1024                 0.524133  0.371204  0.409127    0.353783   \n",
       "distilbert-base-uncased           0.504549  0.329222  0.376833    0.316757   \n",
       "albert-large-v2                   0.480543  0.307515  0.354598    0.292581   \n",
       "allenai/scibert_scivocab_uncased  0.485229  0.323698  0.363979    0.305105   \n",
       "vinai/phobert-large               0.486945  0.320247  0.364166    0.303000   \n",
       "\n",
       "                                  Rouge1_r  Rouge2_r  RougeL_r  RougeSU4_r  \\\n",
       "bert-base-multilingual-uncased    0.304166  0.211028  0.231594    0.198776   \n",
       "bert-large-uncased                0.258243  0.176876  0.195496    0.165626   \n",
       "gpt2                              0.267464  0.189232  0.209020    0.178608   \n",
       "facebook/bart-large               0.294335  0.207849  0.230653    0.197494   \n",
       "openai-gpt                        0.267715  0.175316  0.201075    0.165604   \n",
       "ctrl                              0.305062  0.211551  0.223330    0.199203   \n",
       "transfo-xl-wt103                  0.245627  0.168283  0.186899    0.156607   \n",
       "xlnet-large-cased                 0.294641  0.215823  0.229381    0.200726   \n",
       "xlm-mlm-enfr-1024                 0.306791  0.223627  0.241264    0.209859   \n",
       "distilbert-base-uncased           0.288143  0.195924  0.215930    0.185842   \n",
       "albert-large-v2                   0.256889  0.165319  0.190731    0.155124   \n",
       "allenai/scibert_scivocab_uncased  0.268949  0.178652  0.200715    0.165867   \n",
       "vinai/phobert-large               0.307452  0.208641  0.230677    0.194424   \n",
       "\n",
       "                                  Rouge1_f  Rouge2_f  RougeL_f  RougeSU4_f  \n",
       "bert-base-multilingual-uncased    0.378443  0.259207  0.286948    0.244950  \n",
       "bert-large-uncased                0.341384  0.233719  0.258361    0.220315  \n",
       "gpt2                              0.357890  0.252715  0.279927    0.240039  \n",
       "facebook/bart-large               0.378934  0.264659  0.296998    0.252698  \n",
       "openai-gpt                        0.345712  0.225798  0.260565    0.214921  \n",
       "ctrl                              0.381038  0.265649  0.280269    0.251660  \n",
       "transfo-xl-wt103                  0.334856  0.227786  0.254359    0.213378  \n",
       "xlnet-large-cased                 0.375959  0.273854  0.292148    0.256065  \n",
       "xlm-mlm-enfr-1024                 0.387038  0.279109  0.303533    0.263446  \n",
       "distilbert-base-uncased           0.366806  0.245656  0.274543    0.234249  \n",
       "albert-large-v2                   0.334800  0.215036  0.248044    0.202751  \n",
       "allenai/scibert_scivocab_uncased  0.346077  0.230235  0.258746    0.214904  \n",
       "vinai/phobert-large               0.376920  0.252668  0.282443    0.236862  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = category[5]\n",
    "\n",
    "pathbody = 'E:/TextSummarization/donvanban/Plaintext/' + idx \n",
    "pathref = 'E:/TextSummarization/donvanban/Summary_manual/' + idx \n",
    "rootbody = loadtxt(pathbody)\n",
    "refbody = loadtxt(pathref, ref=True)\n",
    "\n",
    "res = {'Rouge1_p':[], 'Rouge2_p':[], 'RougeL_p':[], 'RougeSU4_p': [],\n",
    "        'Rouge1_r':[], 'Rouge2_r':[], 'RougeL_r':[], 'RougeSU4_r': [],\n",
    "        'Rouge1_f':[], 'Rouge2_f':[], 'RougeL_f':[], 'RougeSU4_f': [],\n",
    "      }\n",
    "\n",
    "for idx in model_dict.keys():    \n",
    "    model = Summarizer(model = idx, sentence_handler=CoreferenceHandler())\n",
    "    all_result = []\n",
    "    for jdx in range(len(rootbody)):\n",
    "        result = model(rootbody[jdx], ratio=2*len(refbody[jdx])/len(rootbody[jdx]))\n",
    "        all_result.append(result)\n",
    "    r_rouge = rouge_dist(all_result, refbody)\n",
    "    for ind in ['p', 'r', 'f']:\n",
    "        res['Rouge1_'+ind].append(r_rouge['rouge-1'][ind])\n",
    "        res['Rouge2_'+ind].append(r_rouge['rouge-2'][ind])\n",
    "        res['RougeL_'+ind].append(r_rouge['rouge-l'][ind])\n",
    "        res['RougeSU4_'+ind].append(r_rouge['rouge-su4'][ind])\n",
    "df = pd.DataFrame(res, index = model_dict.keys())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     10
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Summarizer(\n",
    "    model: This gets used by the hugging face bert library to load the model, you can supply a custom trained model here\n",
    "    custom_model: If you have a pre-trained model, you can add the model class here.\n",
    "    custom_tokenizer:  If you have a custom tokenizer, you can add the tokenizer here.\n",
    "    hidden: Needs to be negative, but allows you to pick which layer you want the embeddings to come from.\n",
    "    reduce_option: It can be 'mean', 'median', or 'max'. This reduces the embedding layer for pooling.\n",
    "    sentence_handler: The handler to process sentences. If want to use coreference, instantiate and pass CoreferenceHandler instance)\n",
    "model(body: str # The string body that you want to summarize\n",
    "    ratio: float # The ratio of sentences that you want for the final summary\n",
    "    min_length: int # Parameter to specify to remove sentences that are less than 40 characters\n",
    "    max_length: int # Parameter to specify to remove sentences greater than the max length,\n",
    "    num_sentences: Number of sentences to use. Overrides ratio if supplied)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T05:43:08.778428Z",
     "start_time": "2021-07-22T05:43:08.767458Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "body = '''\n",
    "Shiba Inu ( (si khuyn)) l loi ch nh nht trong su ging ch nguyn thy v ring bit n t Nht Bn. Chng l mt ging ch nh, nhanh nhn v thch hp vi a hnh min ni, Shiba Inu ban u c nui  sn bt. N gn ging nhng nh hn so vi ging Akita Inu. y l mt trong s t ging ch c xa vn cn tn ti cho n ngy nay.\n",
    "Shiba l mt trong su ging ch in hnh ca Nht Bn, cng nh Hokkaido, Kishu, Shikoku, Kai v Akita. Trong nhng ging ch ny, Shiba l nh nht.\n",
    "Inu hoc ken ( - Hn Vit: khuyn) trong ting Nht c ngha l con ch, nhng ngun gc ca t \"Shiba\" vn cha r. T Shiba ( - Hn Vit: si) c ngha l cy bi trong ting Nht,  cp n mt loi cy hoc cy bi c l chuyn sang mu  vo ma thu. iu ny khin cho mt s ngi tin rng Shiba c t tn nh th l v loi ch ny c s dng  sn mi trong cc bi cy, hoc c th l do mu sc ph bin nht ca Shiba Inu l mu  tng t nh ca cc cy bi. Tuy nhin, trong mt phng ng Nagano c, t Shiba cng c  ngha l nh, do  ci tn c th ni n tm vc nh b ca con ch. Do , Shiba Inu i khi c dch l \"Little Brushwood Dog\", tc \"Ch bi nh\".\n",
    "Khung hnh ca Shiba nh gn vi c bp pht trin tt. Con c c chiu cao t 35 n 43 cm (14 n 17 in). i vi con ci l 33 n 41 cm (13 n 16 in). Trng lng trung bnh  kch thc tng thch l khong 10 kg (22 lb) i vi con c v 8 kg (18 lb) i vi con ci. Xng va phi.\n",
    "Lp lng: C hai lp lng vi lp ngoi cng v thng cng mt lp trong mm mi v dy. Lng mao ngn v thm ch trn mt, tai v chn ging co. Lng bo v x ra khi c th chiu di khong 4 n 5 cm (1 12 n 2 in)  vai. Lng ui hi di v x ra. Shiba c th c mu , en v nu, hoc mu vng (mu  vi nhng si ng sang en), vi mt lp lng lt mu kem, mu da b, hoc mu xm. N cng c th c mu trng (kem), mc d mu ny c xem l mt\"li nghim trng\"bi Hip hi ch ging M v khng bao gi c nui trong cc chng trnh. Ngc li, mt lp lng mu trng (kem) l hon ton chp nhn c theo tiu chun ging ch Anh.\n",
    "Urajiro (mu kem trng) c  cc b phn sau trn tt c cc vng lng:  hai bn mm, trn m, bn trong tai, trn hm di v  ch c hng, bn trong chn, trn bng, xung quanh cc l thng hi v pha vng bng ca ui. Mu : thng  trn c hng, chp ngc v ngc. en v mu vng: thng l mt du tam gic trn c hai bn ca chp ngc.\n",
    "Shiba c xu hng th hin tnh t lp v i khi cn hung hng. Shiba Inu tt nht nn c nui trong mt gia nh m khng c nhng con ch nh khc hay tr em, nhng hun luyn vng li vn c th c v x hi sm c th lm cho tt c tr nn ngoan ngon. Ging ch cng tng tc kh tt vi mo.\n",
    "Mt tinh thn mnh dn, mt bn cht tt p v s thng thn khng ln ln mang li phm gi v v p t nhin. Shiba c tnh cht t lp v c th d dt i vi ngi l nhng li trung thnh v tnh cm vi nhng ngi c c s tn trng ca n. N c th hung d vi nhng con ch khc.\n",
    "Shiba l mt ging ch tng i kh tnh v cm thy rt cn thit khi gi chnh n tht sch. N thng lim bn chn ging nh mo, thng di chuyn theo cch ring ca mnh  gi b lng sch s, nhng li cc k thch bi li v chi a trong cc vng nc. V bn cht kh tnh v y kiu hnh vn c, Shiba con rt d dy d v trong nhiu trng hp s t dy d chnh mnh. Ch cn ch n gin l t chng ra ngoi sau gi n v ng th c th ni l    dy Shiba phng php thch hp  i v sinh.\n",
    "Mt c im gip phn bit ging ch ny l \"Shiba scream\". Khi  kch ng hay khng vui, n s pht ra mt ting tht ln v cao. iu ny c th xy ra khi n c gng  x l con ch theo mt cch m n cho l khng th chp nhn c. Cc ng vt khc cng c th pht ra m thanh tng t nh trong nhng lc vui, chng hn nh s tr li ca ch nhn sau khi vng mt lu ngy hay s xut hin ca mt ngi khch yu thch,...\n",
    "Th nghim phn tch DNA gn y  khng nh rng loi ch mm nhn chu  ny l mt trong nhng ging ch lu i nht,  sng t th k th 3 trc Cng nguyn.\n",
    "Ban u, Shiba Inu c nui  sn v bt cc con vt nh, chng hn nh cc loi chim v th. D  c nhiu n lc  bo tn ging, Shiba gn b tuyt chng trong Chin tranh th gii th hai do tnh trng thiu thc phm cng thm dch bnh ch sau chin tranh. Tt c nhng con ch sau ny c to ra ch t ba dng mu cn sng st. Nhng dng mu  l Shinshu Shiba t Nagano, Mino Shiba t Gifu, v San'in Shiba t Tottori v Shimane. Shinshu Shiba s hu mt lp lng t rn, vi mt lp lng dy bo v, nh v c mu . Mino Shiba thng c i tai dy, nhn v s hu mt ci ui hnh li lim, ch khng phi l ui cun trn thng c tm thy trn Shiba hin nay. San'in Shiba th ln hn so vi hu ht cc ging Shiba hin nay, v thng c mu en, khng c du sm v trng thng c tm thy trn Shiba en - sm hin nay. Khi nghin cu v ch Nht c chnh thc ha trong u v gia th k 20, ba chng ny  c kt hp thnh mt ging tng th, Shiba Inu. Cc tiu chun ging Nht Bn u tin cho Shiba, tiu chun Nippo, c xut bn vo nm 1934. Vo thng 12 nm 1936, cc Shiba Inu c cng nhn l Di tch t nhin ca Nht Bn thng qua o lut vn ha, phn ln l do nhng n lc ca Nippo (Nihon Ken Hozonkai) - Hip hi Bo tn Ch Nht Bn.\n",
    "Nm 1954, mt gia nh phc v v trang mang con Shiba Inu u tin n Hoa K. Vo nm 1979, la u tin c ghi nhn sinh ra ti Hoa K. Shiba  c cng nhn bi Hip hi ch ging M vo nm 1992 v c b sung vo nhm AKC (nhm phi th thao) vo nm 1993. Ging by gi ch yu c nui nh th cng  Nht Bn v cc nc khc.\n",
    "Mt con Shina Inu ang chi a  bi c.\n",
    "Tnh trng sc khe c bit nh hng n ging ch ny l d ng, thanh quang nhn, cm thy tinh th mt, lon sn xng hng, qup v trt xng bnh ch. Nhn chung, d g i na, chng c tnh di truyn cao v kh nhiu Shiba c chn on khuyt tt do di truyn so vi cc ging ch khc.\n",
    "Kim tra chung nh k c khuyn co nn c thc hin trong sut cuc i ca con ch nhng vn  thng c pht hin sm trong cuc i ca n. Kim tra mt nn c thc hin hng nm v vn  v mt c th pht trin theo thi gian. Nm hai tui, Shiba Inu c th c coi l hon ton t do khi cc vn  chung nu khng c pht hin bi thi im ny, v   tui ny b xng  c pht trin y .\n",
    "Nh i vi bt k nhng con ch khc, Shiba nn c i hoc nu khng th nn vn ng hng ngy.\n",
    "Tui th trung bnh ca Shiba Inu l t 12 n 16 nm. Tp th dc, c bit l i b mi ngy, s gip cho ging ch ny sng lu v khe mnh. Shiba lu i nht c bit n l \"Pusuke\",  qua i  tui 26 vo u thng 12 nm 2011 v l ch ch gi nht cn sng vo thi im .\n",
    "Ging ch ny rt sch s, v vy nhu cu chi chut nn c thc hin ti thiu. Mt lp lng Shiba Inu th, ngn c chiu di trung bnh vi lp lng bn ngoi di 2,5 n 3,2 cm (1 n 1 14 in); v khng thm nc t nhin nn t cn tm thng xuyn. N cng c mt lp lng dy c th bo v chng khi nhit  ng . Tuy nhin, rng lng c th l mt mi phin toi. Rng lng nng nht c s thay i theo ma v c bit l trong ma h, nhng vic chi lng hng ngy c th lm gim vn  ny. Ch nhn khng c php co hoc ct lng ca Shiba Inu, v lng cn thit  bo v ch khi nhit  c nng ln lnh.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T05:48:59.399137Z",
     "start_time": "2021-07-22T05:48:47.838549Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shiba Inu ( (si khuyn)) l loi ch nh nht trong su ging ch nguyn thy v ring bit n t Nht Bn. iu ny khin cho mt s ngi tin rng Shiba c t tn nh th l v loi ch ny c s dng  sn mi trong cc bi cy, hoc c th l do mu sc ph bin nht ca Shiba Inu l mu  tng t nh ca cc cy bi. i vi con ci l 33 n 41 cm (13 n 16 in). Shiba c th c mu , en v nu, hoc mu vng (mu  vi nhng si ng sang en), vi mt lp lng lt mu kem, mu da b, hoc mu xm. Urajiro (mu kem trng) c  cc b phn sau trn tt c cc vng lng:  hai bn mm, trn m, bn trong tai, trn hm di v  ch c hng, bn trong chn, trn bng, xung quanh cc l thng hi v pha vng bng ca ui. Shiba c tnh cht t lp v c th d dt i vi ngi l nhng li trung thnh v tnh cm vi nhng ngi c c s tn trng ca n. N c th hung d vi nhng con ch khc. Khi nghin cu v ch Nht c chnh thc ha trong u v gia th k 20, ba chng ny  c kt hp thnh mt ging tng th, Shiba Inu. Nhn chung, d g i na, chng c tnh di truyn cao v kh nhiu Shiba c chn on khuyt tt do di truyn so vi cc ging ch khc. Tuy nhin, rng lng c th l mt mi phin toi.\n"
     ]
    }
   ],
   "source": [
    "#default model with number of senteces = 10\n",
    "model = Summarizer()\n",
    "result = model(body, num_sentences = 10)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T05:47:07.735002Z",
     "start_time": "2021-07-22T05:46:35.772266Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shiba Inu ( (si khuyn)) l loi ch nh nht trong su ging ch nguyn thy v ring bit n t Nht Bn. Tuy nhin, trong mt phng ng Nagano c, t Shiba cng c  ngha l nh, do  ci tn c th ni n tm vc nh b ca con ch. Khung hnh ca Shiba nh gn vi c bp pht trin tt. Mt tinh thn mnh dn, mt bn cht tt p v s thng thn khng ln ln mang li phm gi v v p t nhin. N c th hung d vi nhng con ch khc. N thng lim bn chn ging nh mo, thng di chuyn theo cch ring ca mnh  gi b lng sch s, nhng li cc k thch bi li v chi a trong cc vng nc. V bn cht kh tnh v y kiu hnh vn c, Shiba con rt d dy d v trong nhiu trng hp s t dy d chnh mnh. Ging by gi ch yu c nui nh th cng  Nht Bn v cc nc khc. Tp th dc, c bit l i b mi ngy, s gip cho ging ch ny sng lu v khe mnh. Ging ch ny rt sch s, v vy nhu cu chi chut nn c thc hin ti thiu. N cng c mt lp lng dy c th bo v chng khi nhit  ng .\n"
     ]
    }
   ],
   "source": [
    "#BERT large Uncased model with number of senteces = 10\n",
    "model = Summarizer(model = 'bert-large-uncased')\n",
    "result = model(body, num_sentences = 10)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T05:47:41.476382Z",
     "start_time": "2021-07-22T05:47:07.762908Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shiba Inu ( (si khuyn)) l loi ch nh nht trong su ging ch nguyn thy v ring bit n t Nht Bn. Inu hoc ken ( - Hn Vit: khuyn) trong ting Nht c ngha l con ch, nhng ngun gc ca t \"Shiba\" vn cha r. Khung hnh ca Shiba nh gn vi c bp pht trin tt. i vi con ci l 33 n 41 cm (13 n 16 in). Ging ch cng tng tc kh tt vi mo. N c th hung d vi nhng con ch khc. N thng lim bn chn ging nh mo, thng di chuyn theo cch ring ca mnh  gi b lng sch s, nhng li cc k thch bi li v chi a trong cc vng nc. V bn cht kh tnh v y kiu hnh vn c, Shiba con rt d dy d v trong nhiu trng hp s t dy d chnh mnh. Mt c im gip phn bit ging ch ny l \"Shiba scream\". Nhng dng mu  l Shinshu Shiba t Nagano, Mino Shiba t Gifu, v San'in Shiba t Tottori v Shimane. San'in Shiba th ln hn so vi hu ht cc ging Shiba hin nay, v thng c mu en, khng c du sm v trng thng c tm thy trn Shiba en - sm hin nay. Vo nm 1979, la u tin c ghi nhn sinh ra ti Hoa K. Ging by gi ch yu c nui nh th cng  Nht Bn v cc nc khc. Kim tra chung nh k c khuyn co nn c thc hin trong sut cuc i ca con ch nhng vn  thng c pht hin sm trong cuc i ca n. Nm hai tui, Shiba Inu c th c coi l hon ton t do khi cc vn  chung nu khng c pht hin bi thi im ny, v   tui ny b xng  c pht trin y . Nh i vi bt k nhng con ch khc, Shiba nn c i hoc nu khng th nn vn ng hng ngy. Tp th dc, c bit l i b mi ngy, s gip cho ging ch ny sng lu v khe mnh. Ging ch ny rt sch s, v vy nhu cu chi chut nn c thc hin ti thiu. N cng c mt lp lng dy c th bo v chng khi nhit  ng . Tuy nhin, rng lng c th l mt mi phin toi. Rng lng nng nht c s thay i theo ma v c bit l trong ma h, nhng vic chi lng hng ngy c th lm gim vn  ny.\n"
     ]
    }
   ],
   "source": [
    "#BERT large Uncased model with ratio = 0.3\n",
    "model = Summarizer(model = 'bert-large-uncased')\n",
    "result = model(body, ratio = 0.3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T05:48:18.093033Z",
     "start_time": "2021-07-22T05:47:41.502313Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shiba Inu ( (si khuyn)) l loi ch nh nht trong su ging ch nguyn thy v ring bit n t Nht Bn. Chng l mt ging ch nh, nhanh nhn v thch hp vi a hnh min ni, Shiba Inu ban u c nui  sn bt. Inu hoc ken ( - Hn Vit: khuyn) trong ting Nht c ngha l con ch, nhng ngun gc ca t \"Shiba\" vn cha r. Khung hnh ca Shiba nh gn vi c bp pht trin tt. Con c c chiu cao t 35 n 43 cm (14 n 17 in). Trng lng trung bnh  kch thc tng thch l khong 10 kg (22 lb) i vi con c v 8 kg (18 lb) i vi con ci. Ging ch cng tng tc kh tt vi mo. N c th hung d vi nhng con ch khc. N thng lim bn chn ging nh mo, thng di chuyn theo cch ring ca mnh  gi b lng sch s, nhng li cc k thch bi li v chi a trong cc vng nc. Mt c im gip phn bit ging ch ny l \"Shiba scream\". Khi  kch ng hay khng vui, n s pht ra mt ting tht ln v cao. Nm 1954, mt gia nh phc v v trang mang con Shiba Inu u tin n Hoa K. Vo nm 1979, la u tin c ghi nhn sinh ra ti Hoa K. Ging by gi ch yu c nui nh th cng  Nht Bn v cc nc khc. Kim tra chung nh k c khuyn co nn c thc hin trong sut cuc i ca con ch nhng vn  thng c pht hin sm trong cuc i ca n. Nm hai tui, Shiba Inu c th c coi l hon ton t do khi cc vn  chung nu khng c pht hin bi thi im ny, v   tui ny b xng  c pht trin y . Nh i vi bt k nhng con ch khc, Shiba nn c i hoc nu khng th nn vn ng hng ngy. Tp th dc, c bit l i b mi ngy, s gip cho ging ch ny sng lu v khe mnh. Ging ch ny rt sch s, v vy nhu cu chi chut nn c thc hin ti thiu. Tuy nhin, rng lng c th l mt mi phin toi. Rng lng nng nht c s thay i theo ma v c bit l trong ma h, nhng vic chi lng hng ngy c th lm gim vn  ny.\n"
     ]
    }
   ],
   "source": [
    "#default model with CoreferenceHandler\n",
    "model = Summarizer(model = 'bert-large-uncased', sentence_handler=CoreferenceHandler())\n",
    "result = model(body, ratio = 0.3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T05:50:27.372771Z",
     "start_time": "2021-07-22T05:50:08.993668Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\transformers\\configuration_xlnet.py:212: FutureWarning: This config doesn't use attention memories, a core feature of XLNet. Consider setting `men_len` to a non-zero value, for example `xlnet = XLNetLMHeadModel.from_pretrained('xlnet-base-cased'', mem_len=1024)`, for accurate training performance as well as an order of magnitude faster inference. Starting from version 3.5.0, the default parameter will be 1024, following the implementation in https://arxiv.org/abs/1906.08237\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shiba Inu ( (si khuyn)) l loi ch nh nht trong su ging ch nguyn thy v ring bit n t Nht Bn. T Shiba ( - Hn Vit: si) c ngha l cy bi trong ting Nht,  cp n mt loi cy hoc cy bi c l chuyn sang mu  vo ma thu. iu ny khin cho mt s ngi tin rng Shiba c t tn nh th l v loi ch ny c s dng  sn mi trong cc bi cy, hoc c th l do mu sc ph bin nht ca Shiba Inu l mu  tng t nh ca cc cy bi. Khung hnh ca Shiba nh gn vi c bp pht trin tt. Trng lng trung bnh  kch thc tng thch l khong 10 kg (22 lb) i vi con c v 8 kg (18 lb) i vi con ci. Lp lng: C hai lp lng vi lp ngoi cng v thng cng mt lp trong mm mi v dy. Mu : thng  trn c hng, chp ngc v ngc. Shiba c xu hng th hin tnh t lp v i khi cn hung hng. Shiba Inu tt nht nn c nui trong mt gia nh m khng c nhng con ch nh khc hay tr em, nhng hun luyn vng li vn c th c v x hi sm c th lm cho tt c tr nn ngoan ngon. Ging ch cng tng tc kh tt vi mo. N c th hung d vi nhng con ch khc. Shiba l mt ging ch tng i kh tnh v cm thy rt cn thit khi gi chnh n tht sch. Nm 1954, mt gia nh phc v v trang mang con Shiba Inu u tin n Hoa K. Ging by gi ch yu c nui nh th cng  Nht Bn v cc nc khc. Mt con Shina Inu ang chi a  bi c. Tnh trng sc khe c bit nh hng n ging ch ny l d ng, thanh quang nhn, cm thy tinh th mt, lon sn xng hng, qup v trt xng bnh ch. Nhn chung, d g i na, chng c tnh di truyn cao v kh nhiu Shiba c chn on khuyt tt do di truyn so vi cc ging ch khc. Nm hai tui, Shiba Inu c th c coi l hon ton t do khi cc vn  chung nu khng c pht hin bi thi im ny, v   tui ny b xng  c pht trin y . Tp th dc, c bit l i b mi ngy, s gip cho ging ch ny sng lu v khe mnh. N cng c mt lp lng dy c th bo v chng khi nhit  ng . Tuy nhin, rng lng c th l mt mi phin toi.\n"
     ]
    }
   ],
   "source": [
    "model = Summarizer(model = 'xlnet-base-cased', sentence_handler=CoreferenceHandler())\n",
    "result = model(body, ratio = 0.3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T05:52:13.079809Z",
     "start_time": "2021-07-22T05:52:02.404352Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06264511, -0.04616188,  0.30910435, ..., -0.11081592,\n",
       "        -0.06871919,  0.11080622],\n",
       "       [ 0.0010872 , -0.0021876 ,  0.04671476, ...,  0.42539126,\n",
       "        -0.261949  ,  0.09410372],\n",
       "       [-0.03221155, -0.21116859,  0.19516714, ..., -0.12356774,\n",
       "        -0.13606201,  0.36228496],\n",
       "       ...,\n",
       "       [-0.4348117 ,  0.3021262 ,  0.21845956, ..., -0.6307475 ,\n",
       "        -0.06835769, -0.02482057],\n",
       "       [-0.21573763,  0.32624435, -0.06969851, ..., -0.20903395,\n",
       "        -0.34038976, -0.0154977 ],\n",
       "       [-0.18740307, -0.29025468, -0.01592006, ..., -0.175454  ,\n",
       "         0.24482788,  0.2545929 ]], dtype=float32)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.run_embeddings(body, ratio=0.3)  # Will return (num_sentences +1, N) embedding numpy matrix.\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T05:52:35.121139Z",
     "start_time": "2021-07-22T05:52:29.509356Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2692.918701171875,\n",
       " 2500.3896484375,\n",
       " 2321.54345703125,\n",
       " 2202.852783203125,\n",
       " 2082.740966796875,\n",
       " 2033.31689453125,\n",
       " 1945.07568359375,\n",
       " 1863.1490478515625,\n",
       " 1775.525390625]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model.calculate_elbow(body, k_max=10)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T05:52:40.701998Z",
     "start_time": "2021-07-22T05:52:35.153047Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model.calculate_optimal_k(body, k_max=10)\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
